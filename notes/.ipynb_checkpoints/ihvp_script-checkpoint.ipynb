{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toy example을 제외하고, 코드 구현, 결과를 통합해서 visualization하기 위한 script\n",
    "\n",
    "다루고 있는 것으로는\n",
    "\n",
    "# Code Implementation\n",
    "- HVP\n",
    "- IHVP-CG\n",
    "- IHVP-NCG\n",
    "- IHVP-SE\n",
    "\n",
    "# Experimental Result\n",
    "\n",
    "- TOP-k examples sorted by IF measure\n",
    "- t-sne result\n",
    "- interpretation of result\n",
    "- TOP-k examples sorted by IF measure on specific class\n",
    "- relabeling using IF measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HVP\n",
    "\n",
    "Remark)\n",
    "\n",
    "tensorflow에서는 gradient가 operator로 존재하며 이를 이용하면 hessian vector product를 automatic differentiation으로 구현할 수 있음. 그 결과 추가적인 error 없이 정확한 값을 얻을 수 있음.\n",
    "\n",
    "반면 cntk에서는 gradient output이 값으로만 나오게 됨. 때문에 numerical differentiation을 이용해서 구현함. 그 결과 error가 조금 발생하게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.device import try_set_default_device, gpu\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hessian Vector Product\n",
    "\n",
    "def grad_inner_product(grad1, grad2):\n",
    "    # inner product for dictionary-format gradients (output scalar value)\n",
    "    \n",
    "    val = 0\n",
    "    \n",
    "    assert(len(grad1)==len(grad2))\n",
    "    \n",
    "    for ks in grad1.keys():\n",
    "        val += np.sum(np.multiply(grad1[ks],grad2[ks]))\n",
    "        \n",
    "    return val\n",
    "\n",
    "def weight_update(w, v, r):\n",
    "    # w: weights of neural network (tuple)\n",
    "    # v: value for delta w (dictionary, e.g., gradient value)\n",
    "    # r: hyperparameter for a gradient (scalar)\n",
    "\n",
    "    for p in w:\n",
    "        p.value += r * v[p]\n",
    "\n",
    "def HVP(y, x, v):\n",
    "    # Calculate Hessian vector product \n",
    "    # y: scalar function to be differentiated (function, e.g. cross entropy loss)\n",
    "    # x: feed_dict value for the network (dictionary, e.g. {model.X: image_batch, model.y: label_batch})\n",
    "    # v: vector to be producted (by Hessian) (numeric dictionary, e.g., g(z_test))\n",
    "    ## w: variables to differentiate (numeric, e.g. neural network weight)\n",
    "    \n",
    "    # hyperparameter r\n",
    "    r = 1e-2\n",
    "    \n",
    "    assert type(x)==dict, \"Input of HVP is wrong. 'x' should be dictionary(feed dict format)\"\n",
    "    assert type(v)==dict, \"Input of HVP is wrong. 'v' should be dictionary(weight:value format)\"\n",
    "\n",
    "    w = v.keys()\n",
    "    \n",
    "    # gradient for plus\n",
    "    weight_update(w, v, +r)\n",
    "    g_plus = y.grad(x, wrt=w)\n",
    "  \n",
    "    # gradient for minus\n",
    "    weight_update(w, v, -2*r)\n",
    "    g_minus = y.grad(x, wrt=w)\n",
    "    \n",
    "    # weight reconstruction\n",
    "    weight_update(w, v, +r)\n",
    "    \n",
    "    hvp = {ks: (g_plus[ks] - g_minus[ks])/(2*r) for ks in g_plus.keys()}\n",
    "       \n",
    "    return hvp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHVP\n",
    "\n",
    "논문에서 나온 conjugate gradient와 stochastic estimation 두 방법을 모두 구현함.\n",
    "\n",
    "Remark)\n",
    "\n",
    "원작자 코드에서는 conjugate gradient를 구현할 때 scipy의 ncg (Newton's conjugate gradient)를 사용해서 구현함. 하지만 ncg의 경우 update를 담당하는 outer loop 안에 update 수치를 찾기 위한 작은 inner loop를 하나 더 돌게 되는데, 적절한 해를 찾지 못할 경우 이 inner loop를 벗어나지 못할 가능성이 있음.\n",
    "- 과거 버전에서는 while loop을 사용해서 진행되어 평생 벗어나지 못할 가능성이 있음.\n",
    "- 최신 버전에서는 for loop을 사용해서 진행되어 cg_maxiter를 넘기면 벗어날 가능성이 있으나 이 값은 내부에서만 존재하는 hyperparameter라서 바꿔줄 수 없음. 내부에서 지정된 값은 20 * len(x0)인데, 우리의 경우 len(x0)가 80000이 넘어가기 때문에 사실상 며칠을 돌려도 끝나지 않음. (몇 십분 돌렸을 때 겨우 400번 정도 돌았음.)\n",
    "\n",
    "때문에 scipy의 cg를 사용해서 구현함. 이 경우 maxiter를 이용하면 수렴하지 않더라도 학습을 중간에 끝낼 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conjugate Gradient\n",
    "from scipy.optimize import fmin_ncg, fmin_cg\n",
    "\n",
    "def dic2vec(dic):\n",
    "    # convert a dictionary with matrix values to a 1D vector\n",
    "    # e.g. gradient of network -> 1D vector\n",
    "    vec = np.concatenate([val.reshape(-1) for val in dic.values()])\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def vec2dic(vec, fmt):\n",
    "    # convert a 1D vector to a dictionary of format fmt\n",
    "    # fmt: {key: val.shape for (key,val) in dict}\n",
    "    fmt_idx = [np.prod(val) for val in fmt.values()]\n",
    "    vec_split = [vec[sum(fmt_idx[:i]):sum(fmt_idx[:i+1])] for i in range(len(fmt_idx))]\n",
    "    dic = {key: vec_split[i].reshape(shape) for (i,(key,shape)) in enumerate(fmt.items())}\n",
    "\n",
    "    return dic\n",
    "\n",
    "def get_inverse_hvp_cg(model, y, v, data_set, method='Basic', **kwargs):\n",
    "    # Calculate inverse hessian vector product over the training set using CG method\n",
    "    # return x, which is the solution of QP, whose value is H^-1 v\n",
    "    # model: neural network model (e.g. model)\n",
    "    # y: scalar function output of the neural network (e.g. model.loss)\n",
    "    # v: vector to be producted by inverse hessian (i.e.H^-1 v) (e.g. v_test)\n",
    "    # data_set: training set to be summed in Hessian\n",
    "    # method: Basic-> Conjugate Gradient, Newton -> Newton-Conjugate Gradient\n",
    "    # kwargs: hyperparameters for conjugate gradient\n",
    "\n",
    "    # hyperparameters\n",
    "    batch_size = kwargs.pop('batch_size', 50)\n",
    "    damping = kwargs.pop('damping', 0.0)\n",
    "    avextol = kwargs.pop('avextol', 1e-8)\n",
    "    maxiter = kwargs.pop('maxiter', 1e1)\n",
    "    num_workers = kwargs.pop('num_workers', 6)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    get_inverse_hvp_cg.cnt = 0\n",
    "\n",
    "    def HVP_minibatch_val(y, v):\n",
    "        # Calculate Hessian vector product w.r.t whole dataset\n",
    "        # y: scalar function output of the neural network (e.g. model.loss)\n",
    "        # v: vector to be producted by inverse hessian (i.e.H^-1 v) (numeric dictionary, e.g. v_test)\n",
    "        \n",
    "        ## model: neural network model (e.g. model)\n",
    "        ## dataloader: dataloader for the training set\n",
    "        ## damping: damp term to make hessian convex\n",
    "\n",
    "        num_data = data_set.__len__()\n",
    "\n",
    "        hvp_batch = {key: np.zeros_like(value) for key,value in v.items()}\n",
    "\n",
    "        for img, lb in dataloader:\n",
    "            img = img.numpy(); lb = lb.numpy()\n",
    "            x_feed = {model.X: img, model.y:lb}\n",
    "            hvp = HVP(y,x_feed,v)\n",
    "            # add hvp value\n",
    "            for ks in hvp.keys():\n",
    "                hvp_batch[ks] += hvp[ks]/num_data\n",
    "\n",
    "        return hvp_batch\n",
    "\n",
    "    def fmin_loss_fn(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "\n",
    "        return 0.5 * grad_inner_product(hvp_val, x_dic) - grad_inner_product(v, x_dic)\n",
    "\n",
    "    def fmin_grad_fn(x):\n",
    "        # x: 1D vector\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "        v_flat = dic2vec(v)\n",
    "\n",
    "        return hvp_flat - v_flat\n",
    "    \n",
    "    def fmin_hvp_fn(x, p):\n",
    "        p_dic = vec2dic(p, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, p_dic)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "\n",
    "        return hvp_flat\n",
    "\n",
    "    def cg_callback(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        print('iteration: {}'.format(get_inverse_hvp_cg.cnt), ', ', time.time()-t0, '(sec) elapsed')\n",
    "        print('vector element-wise square: ', grad_inner_product(x_dic, x_dic))\n",
    "        get_inverse_hvp_cg.cnt += 1\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    if method == 'Newton':\n",
    "        fmin_results = fmin_ncg(\\\n",
    "                f = fmin_loss_fn, x0 = dic2vec(v), fprime = fmin_grad_fn,\\\n",
    "                fhess_p = fmin_hvp_fn, avextol = avextol, maxiter = maxiter, callback=cg_callback)\n",
    "    else:\n",
    "        fmin_results = fmin_cg(\\\n",
    "                f = fmin_loss_fn, x0 = dic2vec(v), fprime = fmin_grad_fn,\\\n",
    "                maxiter = maxiter, callback = cg_callback)\n",
    "    \n",
    "    return vec2dic(fmin_results, {key: val.shape for (key, val) in v.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Estimation\n",
    "\n",
    "def get_inverse_hvp_se(model, y, v, data_set, **kwargs):\n",
    "    # Calculate inverse hessian vector product over the training set\n",
    "    # model: neural network model (e.g. model)\n",
    "    # y: scalar function output of the neural network (e.g. model.loss)\n",
    "    # v: vector to be producted by inverse hessian (i.e.H^-1 v) (e.g. v_test)\n",
    "    # data_set: training set to be summed in Hessian\n",
    "    # kwargs: hyperparameters for stochastic estimation\n",
    "    \n",
    "    # hyperparameters\n",
    "    recursion_depth = kwargs.pop('recursion_depth', 50) # epoch\n",
    "    scale = kwargs.pop('scale', 1e1) # similar to learning rate\n",
    "    damping = kwargs.pop('damping', 0.0) # paper reference: 0.01\n",
    "    batch_size = kwargs.pop('batch_size', 1)\n",
    "    num_samples = kwargs.pop('num_samples', 1) # the number of samples(:stochatic estimation of IF) to be averaged\n",
    "    tolerance = kwargs.pop('tolerance', 1e-2) # the difference btw l2 norms of current and previous vector used for early stopping\n",
    "    verbose = kwargs.pop('verbose', False)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    inv_hvps = []\n",
    "    \n",
    "    params = v.keys()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # obtain num_samples inverse hvps\n",
    "        cur_estimate = v\n",
    "        prev_norm = 0\n",
    "        \n",
    "        for depth in range(recursion_depth):\n",
    "            # epoch-scale recursion depth\n",
    "            t1 = time.time()\n",
    "            for img, lb in dataloader:\n",
    "                img = img.numpy(); lb = lb.numpy()\n",
    "                x_feed = {model.X: img, model.y:lb}\n",
    "                hvp = HVP(y,x_feed,cur_estimate)\n",
    "                # cur_estimate = v + (1-damping)*cur_estimate + 1/scale*(hvp/batch_size)\n",
    "                cur_estimate = {ks: v[ks] + (1-damping/scale)*cur_estimate[ks] - (1/scale)*hvp[ks]/batch_size for ks in cur_estimate.keys()}\n",
    "            \n",
    "            if verbose:\n",
    "                print('#w: \\n', list(map(lambda x: x.value, params)), '\\n#hvp: \\n', hvp, '\\n#ihvp: \\n', cur_estimate)\n",
    "            \n",
    "            cur_norm = np.sqrt(grad_inner_product(cur_estimate,cur_estimate))\n",
    "            print('Recursion depth: {}, norm: {}, time: {} \\n'.format(depth, cur_norm,time.time()-t1))\n",
    "            \n",
    "            # divergence check\n",
    "            if np.isnan(cur_norm):\n",
    "                print('## The result has been diverged ##')\n",
    "                break\n",
    "            \n",
    "            # convergence check\n",
    "            if np.abs(cur_norm - prev_norm) <= tolerance:\n",
    "                # change this to more precise one (<- scipy.fmin_cg also use gnorm)\n",
    "                print('## Early stopped due to small change')\n",
    "                break\n",
    "            prev_norm = cur_norm\n",
    "        \n",
    "        inv_hvp = {ks: (1/scale)*cur_estimate[ks] for ks in cur_estimate.keys()}\n",
    "        inv_hvps.append(inv_hvp)\n",
    "    \n",
    "    inv_hvp_val = {ks: np.mean([inv_hvps[i][ks] for i in range(num_samples)], axis=0) for ks in inv_hvps[0].keys()}\n",
    "    \n",
    "    return inv_hvp_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example for IHVP (CG, NCG, SE)\n",
    "\n",
    "간단한 neural network를 사용해서 위 알고리즘이 잘 동작하는지 확인.\n",
    "\n",
    "사실 network의 Hessian은 w에 대해서 locally convex함. 따라서 수렴하지 않거나 발산할 가능성이 있음. \n",
    "\n",
    "하지만 w를 고정시켜두고 이를 진행시켰을 때 만약 알고리즘이 locally convex한 경우에서도 잘 동작한다면, (1.25, -0.083) 값이 나와야 함.\n",
    "\n",
    "세 알고리즘 모두 원하는 값에 잘 수렴함을 확인함.\n",
    "(SE의 경우에는 scale에 따라서 발산할 때도 있음.)\n",
    "\n",
    "이에 대한 자세한 결과는 ihvp_toy.ipynb를 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = \n",
      " [[-0.7322467]] \n",
      "w2 = \n",
      " [[-0.68041325]] \n",
      "loss = \n",
      " [  1.25268134e-05]\n",
      "w1 = \n",
      " [[ 1.]] \n",
      "w2 = \n",
      " [[ 0.33333334]] \n",
      "loss = \n",
      " [ 0.1111111]\n",
      "hvp {Parameter('W', [], [1 x 1]): array([[ 2.22302079]], dtype=float32), Parameter('W', [], [1 x 1]): array([[ 9.33413506]], dtype=float32)}\n",
      "######## damping = 0.0, desired solution: [1.25, -0.08] ########\n",
      "iteration: 0 ,  0.19780492782592773 (sec) elapsed\n",
      "vector element-wise square:  0.725185762087\n",
      "iteration: 1 ,  0.36102795600891113 (sec) elapsed\n",
      "vector element-wise square:  1.56326942006\n",
      "iteration: 2 ,  0.5301332473754883 (sec) elapsed\n",
      "vector element-wise square:  1.56095527625\n",
      "iteration: 3 ,  0.7333381175994873 (sec) elapsed\n",
      "vector element-wise square:  1.56935306871\n",
      "iteration: 4 ,  1.154935359954834 (sec) elapsed\n",
      "vector element-wise square:  1.56935306871\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -0.583328\n",
      "         Iterations: 5\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 13\n",
      "         Hessian evaluations: 7\n",
      "iteration: 0 ,  0.15653634071350098 (sec) elapsed\n",
      "vector element-wise square:  0.728215234427\n",
      "iteration: 1 ,  0.3147885799407959 (sec) elapsed\n",
      "vector element-wise square:  1.01896015648\n",
      "iteration: 2 ,  0.40270185470581055 (sec) elapsed\n",
      "vector element-wise square:  1.48828097619\n",
      "iteration: 3 ,  0.5535626411437988 (sec) elapsed\n",
      "vector element-wise square:  1.49732045038\n",
      "iteration: 4 ,  0.6414909362792969 (sec) elapsed\n",
      "vector element-wise square:  1.54981730226\n",
      "iteration: 5 ,  0.8012270927429199 (sec) elapsed\n",
      "vector element-wise square:  1.55726212729\n",
      "iteration: 6 ,  0.8851094245910645 (sec) elapsed\n",
      "vector element-wise square:  1.56873698253\n",
      "iteration: 7 ,  1.0543289184570312 (sec) elapsed\n",
      "vector element-wise square:  1.56880670181\n",
      "iteration: 8 ,  1.1391658782958984 (sec) elapsed\n",
      "vector element-wise square:  1.56923137046\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.583328\n",
      "         Iterations: 9\n",
      "         Function evaluations: 31\n",
      "         Gradient evaluations: 22\n",
      "Recursion depth: 0, norm: 2.0731133858071047, time: 0.04673576354980469 \n",
      "\n",
      "Recursion depth: 1, norm: 2.6626400659997302, time: 0.046655893325805664 \n",
      "\n",
      "Recursion depth: 2, norm: 3.244061702753529, time: 0.04746842384338379 \n",
      "\n",
      "Recursion depth: 3, norm: 3.809035196396589, time: 0.046738386154174805 \n",
      "\n",
      "Recursion depth: 4, norm: 4.34987203133008, time: 0.046753644943237305 \n",
      "\n",
      "Recursion depth: 5, norm: 4.863147647968185, time: 0.04677152633666992 \n",
      "\n",
      "Recursion depth: 6, norm: 5.34788244924037, time: 0.04695487022399902 \n",
      "\n",
      "Recursion depth: 7, norm: 5.804326157855757, time: 0.04970216751098633 \n",
      "\n",
      "Recursion depth: 8, norm: 6.233333509615927, time: 0.04715323448181152 \n",
      "\n",
      "Recursion depth: 9, norm: 6.6360549460074125, time: 0.047308921813964844 \n",
      "\n",
      "Recursion depth: 10, norm: 7.0137767312517, time: 0.04693126678466797 \n",
      "\n",
      "Recursion depth: 11, norm: 7.367827518631629, time: 0.04652905464172363 \n",
      "\n",
      "Recursion depth: 12, norm: 7.699536134358649, time: 0.04622387886047363 \n",
      "\n",
      "Recursion depth: 13, norm: 8.010198577345262, time: 0.04707813262939453 \n",
      "\n",
      "Recursion depth: 14, norm: 8.30106529647423, time: 0.04693937301635742 \n",
      "\n",
      "Recursion depth: 15, norm: 8.57333418648704, time: 0.04707646369934082 \n",
      "\n",
      "Recursion depth: 16, norm: 8.828145382338601, time: 0.046263694763183594 \n",
      "\n",
      "Recursion depth: 17, norm: 9.06657947931957, time: 0.046401262283325195 \n",
      "\n",
      "Recursion depth: 18, norm: 9.289656948931603, time: 0.0459902286529541 \n",
      "\n",
      "Recursion depth: 19, norm: 9.498341157116805, time: 0.046579599380493164 \n",
      "\n",
      "Recursion depth: 20, norm: 9.693541488766066, time: 0.046645402908325195 \n",
      "\n",
      "Recursion depth: 21, norm: 9.876109684088734, time: 0.046216726303100586 \n",
      "\n",
      "Recursion depth: 22, norm: 10.04685144510667, time: 0.04642510414123535 \n",
      "\n",
      "Recursion depth: 23, norm: 10.206517798980322, time: 0.04756307601928711 \n",
      "\n",
      "Recursion depth: 24, norm: 10.355820092623189, time: 0.04699301719665527 \n",
      "\n",
      "Recursion depth: 25, norm: 10.495420559276711, time: 0.04706883430480957 \n",
      "\n",
      "Recursion depth: 26, norm: 10.625942083975422, time: 0.047141075134277344 \n",
      "\n",
      "Recursion depth: 27, norm: 10.747969126433146, time: 0.04628896713256836 \n",
      "\n",
      "Recursion depth: 28, norm: 10.862049548715925, time: 0.04603934288024902 \n",
      "\n",
      "Recursion depth: 29, norm: 10.968695947454139, time: 0.046321868896484375 \n",
      "\n",
      "Recursion depth: 30, norm: 11.068388379316291, time: 0.04699110984802246 \n",
      "\n",
      "Recursion depth: 31, norm: 11.16157781047782, time: 0.047051429748535156 \n",
      "\n",
      "Recursion depth: 32, norm: 11.248683880273477, time: 0.046135902404785156 \n",
      "\n",
      "Recursion depth: 33, norm: 11.330101704946943, time: 0.04664039611816406 \n",
      "\n",
      "Recursion depth: 34, norm: 11.406200585192936, time: 0.04690885543823242 \n",
      "\n",
      "Recursion depth: 35, norm: 11.477325967176789, time: 0.04706573486328125 \n",
      "\n",
      "Recursion depth: 36, norm: 11.54380226675902, time: 0.046468496322631836 \n",
      "\n",
      "Recursion depth: 37, norm: 11.605930880266301, time: 0.04627251625061035 \n",
      "\n",
      "Recursion depth: 38, norm: 11.663993983441175, time: 0.04632282257080078 \n",
      "\n",
      "Recursion depth: 39, norm: 11.718258120068414, time: 0.046792030334472656 \n",
      "\n",
      "Recursion depth: 40, norm: 11.768969862064782, time: 0.047110557556152344 \n",
      "\n",
      "Recursion depth: 41, norm: 11.816361308367048, time: 0.04702305793762207 \n",
      "\n",
      "Recursion depth: 42, norm: 11.860650068148946, time: 0.04651284217834473 \n",
      "\n",
      "Recursion depth: 43, norm: 11.902036737401664, time: 0.04675865173339844 \n",
      "\n",
      "Recursion depth: 44, norm: 11.940711795178949, time: 0.04658985137939453 \n",
      "\n",
      "Recursion depth: 45, norm: 11.976851572428783, time: 0.047032833099365234 \n",
      "\n",
      "Recursion depth: 46, norm: 12.010623441550065, time: 0.04701590538024902 \n",
      "\n",
      "Recursion depth: 47, norm: 12.042180944084341, time: 0.03737282752990723 \n",
      "\n",
      "Recursion depth: 48, norm: 12.071669591765255, time: 0.046967267990112305 \n",
      "\n",
      "Recursion depth: 49, norm: 12.099223953278464, time: 0.04733109474182129 \n",
      "\n",
      "Recursion depth: 50, norm: 12.124971409400187, time: 0.04661083221435547 \n",
      "\n",
      "Recursion depth: 51, norm: 12.149030061357587, time: 0.046170711517333984 \n",
      "\n",
      "Recursion depth: 52, norm: 12.171510889290193, time: 0.03668093681335449 \n",
      "\n",
      "Recursion depth: 53, norm: 12.192515774064129, time: 0.046298980712890625 \n",
      "\n",
      "Recursion depth: 54, norm: 12.21214399277648, time: 0.046317338943481445 \n",
      "\n",
      "Recursion depth: 55, norm: 12.230483823588466, time: 0.04683852195739746 \n",
      "\n",
      "Recursion depth: 56, norm: 12.247618913359396, time: 0.046970367431640625 \n",
      "\n",
      "Recursion depth: 57, norm: 12.26363084946921, time: 0.04642009735107422 \n",
      "\n",
      "Recursion depth: 58, norm: 12.27859131715659, time: 0.04614758491516113 \n",
      "\n",
      "Recursion depth: 59, norm: 12.292569913112551, time: 0.046640872955322266 \n",
      "\n",
      "Recursion depth: 60, norm: 12.305630251176016, time: 0.046733856201171875 \n",
      "\n",
      "Recursion depth: 61, norm: 12.317833280299284, time: 0.04695606231689453 \n",
      "\n",
      "Recursion depth: 62, norm: 12.329234647429148, time: 0.04747939109802246 \n",
      "\n",
      "Recursion depth: 63, norm: 12.339888543328124, time: 0.04645967483520508 \n",
      "\n",
      "Recursion depth: 64, norm: 12.349842443986278, time: 0.04622626304626465 \n",
      "\n",
      "## Early stopped due to small change\n",
      "inverse hvp_ncg {Parameter('W', [], [1 x 1]): array([[ 1.24996459]], dtype=float32), Parameter('W', [], [1 x 1]): array([[-0.0833158]], dtype=float32)} \n",
      "time:  1.1561567783355713\n",
      "inverse hvp_cg {Parameter('W', [], [1 x 1]): array([[ 1.24991691]], dtype=float32), Parameter('W', [], [1 x 1]): array([[-0.08330086]], dtype=float32)} \n",
      "time:  2.105820417404175\n",
      "inverse hvp_se {Parameter('W', [], [1 x 1]): array([[ 1.23245454]], dtype=float32), Parameter('W', [], [1 x 1]): array([[-0.07900628]], dtype=float32)} \n",
      "time:  3.032552719116211\n"
     ]
    }
   ],
   "source": [
    "# toy example for inverse HVP (CG, NCG and SE)\n",
    "\n",
    "class SimpleNet(object):\n",
    "    def __init__(self):\n",
    "        self.X = C.input_variable(shape=(1,))\n",
    "        self.h = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.X)\n",
    "        self.pred = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.h)\n",
    "        self.y = C.input_variable(shape=(1,))\n",
    "        self.loss = C.squared_error(self.pred, self.y)\n",
    "        \n",
    "class SimpleDataset(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self._images, self._labels = images, labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self._images[index]\n",
    "        y = self._labels[index]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "params = net.pred.parameters\n",
    "\n",
    "x_feed = {net.X:np.array([[2.]],dtype=np.float32), net.y:np.array([[1.]],dtype=np.float32)}\n",
    "v_feed = {p: np.ones_like(p.value) for p in params}\n",
    "\n",
    "print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "params[0].value = np.asarray([[1.]])\n",
    "params[1].value = np.asarray([[1./3.]])\n",
    "print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "\n",
    "print('hvp', HVP(net.loss, x_feed, v_feed))\n",
    "\n",
    "#images = np.asarray([[2.],[2.]], dtype=np.float32)\n",
    "#labels = np.asarray([[1.],[1.]], dtype=np.float32)\n",
    "images = np.asarray([[2.]], dtype=np.float32)\n",
    "labels = np.asarray([[1.]], dtype=np.float32)\n",
    "\n",
    "train_set = SimpleDataset(images,labels)\n",
    "\n",
    "print('######## damping = 0.0, desired solution: [1.25, -0.08] ########'); t1 = time.time()\n",
    "ihvp_ncg = get_inverse_hvp_cg(net, net.loss, v_feed, train_set, method='Newton', **{'damping': 0.0}); t2 = time.time()\n",
    "ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_feed, train_set, **{'damping': 0.0}); t3 = time.time()\n",
    "ihvp_se = get_inverse_hvp_se(net, net.loss, v_feed, train_set, **{'damping': 0.0, 'recursion_depth': 100}); t4 = time.time()\n",
    "print('inverse hvp_ncg', ihvp_ncg, '\\ntime: ', t2-t1)\n",
    "print('inverse hvp_cg', ihvp_cg, '\\ntime: ', t3-t2 )\n",
    "print('inverse hvp_se', ihvp_se, '\\ntime: ', t4-t3)\n",
    "\n",
    "# print('inverse hvp_ncg', get_inverse_hvp_ncg(net, net.loss, v_feed, train_set, **{'damping': 0.1}))\n",
    "# print('inverse hvp_cg', get_inverse_hvp_cg(net, net.loss, v_feed, train_set, **{'damping': 0.1}))\n",
    "# print('inverse hvp_se', get_inverse_hvp_se(net, net.loss, v_feed, train_set, **{'scale':10, 'damping':0.1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Result\n",
    "\n",
    "Noisy EMNIST dataset을 사용해서 실험을 진행함.\n",
    "\n",
    "이 데이터를 사용하는 이유는\n",
    "- EMNIST dataset은 일반적으로 사용하는 typo이기 때문에 직관적으로 해석할 수 있음.\n",
    "- noisy label 문제를 다루기 때문에 이와 연결지어 해석할 수 있음.\n",
    "- 과거 학습된 network를 가지고 있음. (suawiki/noisy label 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../refer/datasets-analysis-cntk')\n",
    "import json\n",
    "\n",
    "from datasets import dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.misc\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image_from_data(img, show=True):\n",
    "    # show image from numpy array\n",
    "    # img: (C,W,H) numpy array\n",
    "\n",
    "    if show:\n",
    "        return 0\n",
    "\n",
    "    #img_show = np.squeeze(np.transpose(img,[1,2,0]))\n",
    "    img_show = np.transpose(img,[1,2,0])\n",
    "    imshow(img_show)\n",
    "    plt.show()\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_influence_val(model, ihvp, data_set, cosine=False, **kwargs):\n",
    "    # Calculate influence function value when H^-1 v_test is given w.r.t. data_set\n",
    "    # cf) this will be calculated sample-wisely due to memory issue\n",
    "    \n",
    "    # ihvp: inverse of Hessian Vector Product (dictionary) (e.g. H^-1 v_test)\n",
    "    # data_set: data set to be fed (dataset class) (e.g. train_set)\n",
    "    # kwargs: hyperparameters\n",
    "    \n",
    "    num_workers = kwargs.pop('num_workers', 6)\n",
    "\n",
    "    if_list = []\n",
    "\n",
    "    params = list(ihvp.keys()) # not (model.logits.parameters) due to freezing\n",
    "\n",
    "    num_data = data_set.__len__()\n",
    "    dataloader = DataLoader(data_set, 1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    t1 = time.time()\n",
    "    for img, lb in dataloader:\n",
    "        img = img.numpy(); lb = lb.numpy()\n",
    "        gd = model.loss.grad({model.X:img, model.y:lb}, wrt=params)\n",
    "        # cosine normalization\n",
    "        if cosine:\n",
    "            nrm = np.sqrt(grad_inner_product(gd,gd))\n",
    "            gd = {k: v/nrm for k,v in gd.items()}\n",
    "        if_val = -grad_inner_product(ihvp, gd) / num_data\n",
    "        if_list.append(if_val)\n",
    "    print('get_influence_val takes {} sec'.format(time.time()-t1))\n",
    "\n",
    "    return if_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_topk_samples(measure, data_set, num_sample=5, mask=None,\\\n",
    "        verbose='A/D', show=False, save_path='/Data/result/'):\n",
    "    # measure: list of measure whose each element represents score of each datapoints\n",
    "    # data_set: set of data to be visualized (e.g. train_set)\n",
    "    # num_sample: the number of samples to be visualized\n",
    "    # mask: (0,1) list for training set\n",
    "    # verbose:\n",
    "    #   ALL: show DISADV, ADV, INF, NEG examples\n",
    "    #   A/D: show advantageous and disadvantageous examples\n",
    "    # show: indicator that chooses plt.show or not\n",
    "    \n",
    "    def extract_annotation(data_set, indices, **kwargs):\n",
    "        # extract image annotations from dataset w.r.t. indices\n",
    "        \n",
    "        # data_set: dataset structure\n",
    "        # indices: set of sorted and sampled index (e.g. topk)\n",
    "        # kwargs: other information to be annotated\n",
    "        #   key: name of this feature (str) (e.g. influence function)\n",
    "        #   value: value set of this feature for each datapoints (N x 1 numpy array) (e.g. if value)\n",
    "\n",
    "        images = []; annotations = []\n",
    "\n",
    "        for idx in indices:\n",
    "            img, lb = data_set.__getitem__(idx)\n",
    "            lb_str = data_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "            filename = data_set.filename_list[idx]\n",
    "            \n",
    "            annotation = [\\\n",
    "                    'training set name: {}'.format(filename),\\\n",
    "                    'training set label(anno_dict): {}'.format(lb_str),\\\n",
    "                    ]\n",
    "\n",
    "            for key in kwargs.keys():\n",
    "                annotation.append('{}: {}'.format(key, kwargs[key][idx]))\n",
    "\n",
    "            images.append(img)\n",
    "            annotations.append('\\n'.join(annotation))\n",
    "        \n",
    "        images = np.array(images)\n",
    "\n",
    "        return images, annotations\n",
    "\n",
    "    def draw_images_with_titles(images, filenames, show=False,\\\n",
    "            save_dir='/Data/result/images_with_titles.png'):\n",
    "        \n",
    "        N, Ch, H, W = images.shape\n",
    "\n",
    "        if Ch == 1:\n",
    "            images = np.tile(images, (1,3,1,1))\n",
    "\n",
    "        fig, axes = plt.subplots(N, 1, figsize=(H,W))\n",
    "\n",
    "        for idx in range(N):\n",
    "            image = images[idx].transpose((1,2,0))\n",
    "            filename = filenames[idx]\n",
    "            _ = axes[idx].imshow(image)\n",
    "            _ = axes[idx].axis('off')\n",
    "            _ = axes[idx].set_title(filename)\n",
    "\n",
    "        plt.savefig(os.path.join(save_dir))\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        return 0\n",
    "    \n",
    "    num_data = data_set.__len__()\n",
    "    assert(len(measure)==num_data) \n",
    "\n",
    "    if mask == None:\n",
    "        argsort = np.argsort(measure)\n",
    "    else:\n",
    "        assert(len(mask) == len(measure))\n",
    "        argsort = list(filter(lambda idx: mask[idx], np.argsort(measure)))\n",
    "\n",
    "    topk = argsort[-1:-num_sample-1:-1] # samples that increase loss a lot\n",
    "    botk = argsort[0:num_sample] # samples that decrease loss a lot\n",
    "\n",
    "    # make folder\n",
    "    if os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if verbose == 'A/D' or verbose == 'ALL':\n",
    "        images, annotations = extract_annotation(data_set, topk, **{'measure': measure})\n",
    "        draw_images_with_titles(images, annotations, show=show,\\\n",
    "                save_dir=save_path+'DISADVANTAGEOUS.png')\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def IF_val(net, ihvp, data_set, cosine=False):\n",
    "#     # Calculate influence function w.r.t ihvp and data_set\n",
    "#     # This should be done in sample-wise, since the gradient operation will sum up over whole feed-dicted data\n",
    "    \n",
    "#     # ihvp: inverse hessian vector product (dictionary)\n",
    "#     # data_set: data_set to be feed to the gradient operation (dataset)\n",
    "#     IF_list = []\n",
    "    \n",
    "#     #params = net.logits.parameters\n",
    "#     params = ihvp.keys()\n",
    "    \n",
    "#     dataloader = DataLoader(data_set, 1, shuffle=False, num_workers=6)\n",
    "    \n",
    "#     t1 = time.time()\n",
    "#     for img, lb in dataloader:\n",
    "#         img = img.numpy(); lb = lb.numpy()\n",
    "#         gd = net.loss.grad({net.X:img, net.y:lb}, wrt=params)\n",
    "#         if cosine:\n",
    "#             nrm = np.sqrt(grad_inner_product(gd,gd))\n",
    "#             gd = {k: v/nrm for k,v in gd.items()}\n",
    "#         IF = -grad_inner_product(ihvp, gd) / len(dataloader)\n",
    "#         IF_list.append(IF)\n",
    "#     print('IF_val takes {} sec'.format(time.time()-t1))\n",
    "        \n",
    "#     return IF_list\n",
    "\n",
    "# def visualize_topk_samples(measure, train_set, num_sample=5, mask=None, verbose='ALL', save_path='./result'):\n",
    "#     # 'ALL': show DISADV / ADV / INF / NEG examples\n",
    "#     # 'ADV': show ADV only\n",
    "#     # 'DIS': show DIS only\n",
    "\n",
    "#     axis = 2 # axis=1 -> column output / axis=2 -> row output\n",
    "    \n",
    "#     if mask == None:\n",
    "#         argsort = np.argsort(measure)\n",
    "#     else:\n",
    "#         assert(len(mask) == len(measure))\n",
    "#         argsort = list(filter(lambda idx: mask[idx], np.argsort(measure)))\n",
    "    \n",
    "#     topk = argsort[-1:-num_sample-1:-1]\n",
    "#     botk = argsort[0:num_sample]\n",
    "    \n",
    "#     if not os.path.exists(save_path):\n",
    "#         # make folder\n",
    "#         os.makedirs(save_path)\n",
    "\n",
    "#     if verbose == 'DIS' or verbose == 'ALL':\n",
    "#         dis = []\n",
    "#         true_label = ''; noisy_label = ''\n",
    "#         print('\\n## SHOW {}-MOST DISADVANTAGEOUS EXAMPLES ##\\n'.format(num_sample))\n",
    "#         for idx in topk:\n",
    "#             img, lb = train_set.__getitem__(idx)\n",
    "#             show_image_from_data(img)\n",
    "#             print('training set name: ', train_set.filename_list[idx])\n",
    "#             print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "#             print('IF measure: ', measure[idx])\n",
    "#             print(trainval_list[idx]) # FIXME\n",
    "#             dis.append(img)\n",
    "#             true_label += train_set.filename_list[idx].split('_')[1]\n",
    "#             noisy_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "#         dis = np.squeeze(np.concatenate(dis, axis=axis))\n",
    "#         scipy.misc.imsave(save_path+'/disadvantageous_true_{}_noisy_{}.png'.format(true_label, noisy_label), dis)\n",
    "\n",
    "#     if verbose == 'ADV' or verbose == 'ALL':\n",
    "#         adv = []\n",
    "#         true_label = ''; noisy_label = ''\n",
    "#         print('\\n## SHOW {}-MOST ADVANTAGEOUS EXAMPLES ##\\n'.format(num_sample))\n",
    "#         for idx in botk:\n",
    "#             img, lb = train_set.__getitem__(idx)\n",
    "#             show_image_from_data(img)\n",
    "#             print('training set name: ', train_set.filename_list[idx])\n",
    "#             print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "#             print('IF measure: ', measure[idx])\n",
    "#             print(trainval_list[idx])\n",
    "#             adv.append(img)\n",
    "#             true_label += train_set.filename_list[idx].split('_')[1]\n",
    "#             noisy_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "#         adv = np.squeeze(np.concatenate(adv, axis=axis))\n",
    "#         scipy.misc.imsave(save_path+'/advantageous_true_{}_noisy_{}.png'.format(true_label, noisy_label), adv)\n",
    "\n",
    "    if verbose == 'ALL':\n",
    "        \n",
    "        if mask == None:\n",
    "            argsort_abs = np.argsort(np.abs(measure))\n",
    "        else:\n",
    "            assert(len(mask) == len(measure))\n",
    "            argsort_abs = list(filter(lambda idx: mask[idx], np.argsort(np.abs(measure))))\n",
    "\n",
    "        topk_abs = argsort_abs[-1:-num_sample-1:-1]\n",
    "        botk_abs = argsort_abs[0:num_sample]\n",
    "        \n",
    "        inf = []\n",
    "        true_label = ''; noisy_label = ''\n",
    "        print('\\n## SHOW {}-MOST INFLUENTIAL EXAMPLES ##\\n'.format(num_sample))\n",
    "        for idx in topk_abs:\n",
    "            img, lb = train_set.__getitem__(idx)\n",
    "            show_image_from_data(img)\n",
    "            print('training set name: ', train_set.filename_list[idx])\n",
    "            print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "            print('IF measure: ', measure[idx])\n",
    "            inf.append(img)\n",
    "            true_label += train_set.filename_list[idx].split('_')[1]\n",
    "            noisy_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "        inf = np.squeeze(np.concatenate(inf, axis=axis))\n",
    "        scipy.misc.imsave(save_path+'/influential_true_{}_noisy_{}.png'.format(true_label, noisy_label), inf)\n",
    "\n",
    "        neg = []\n",
    "        true_label = ''; noisy_label = ''\n",
    "        print('\\n## SHOW {}-MOST NEGLIGIBLE EXAMPLES ##\\n'.format(num_sample))\n",
    "        for idx in botk_abs:\n",
    "            img, lb = train_set.__getitem__(idx)\n",
    "            show_image_from_data(img)\n",
    "            print('training set name: ', train_set.filename_list[idx])\n",
    "            print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "            print('IF measure: ', measure[idx])\n",
    "            neg.append(img)\n",
    "            true_label += train_set.filename_list[idx].split('_')[1]\n",
    "            noisy_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "        neg = np.squeeze(np.concatenate(neg, axis=axis))\n",
    "        scipy.misc.imsave(save_path+'/negligible_true_{}_noisy_{}.png'.format(true_label, noisy_label), neg)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 <class 'datasets.dataset.LazyDataset'>\n",
      "500 1 28 28\n"
     ]
    }
   ],
   "source": [
    "# emnist dataset\n",
    "root_dir = '/Data/emnist/balanced/original'\n",
    "\n",
    "# sample size\n",
    "trainval_list, anno_dict = dataset.read_data_subset(root_dir, mode='train1', sample_size=1000)\n",
    "# trainval_list, anno_dict = dataset.read_data_subset(root_dir, mode='train1')\n",
    "test_list, _ = dataset.read_data_subset(root_dir, mode='validation1', sample_size=500)\n",
    "\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict, rescale=False)\n",
    "\n",
    "img, _ = test_set.__getitem__(0)\n",
    "Ch, H, W = img.shape\n",
    "\n",
    "# emnist dataset: SANITY CHECK\n",
    "print(len(test_set), type(test_set))\n",
    "print(len(test_list), Ch, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.shape (32, 28, 28)\n",
      "pool1.shape (32, 14, 14)\n",
      "conv2.shape (48, 14, 14)\n",
      "pool2.shape (48, 7, 7)\n",
      "conv3.shape (64, 7, 7)\n",
      "Test error rate: 0.11599999999999999\n",
      "Total tack time(sec): 0.4635584354400635\n",
      "Tact time per image(sec): 0.000927116870880127\n",
      "Confusion matrix: \n",
      "[[ 9  0  0 ...,  0  0  0]\n",
      " [ 0  3  0 ...,  0  0  0]\n",
      " [ 0  0 14 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ...,  8  0  0]\n",
      " [ 0  0  0 ...,  0 14  0]\n",
      " [ 0  0  0 ...,  0  0 10]]\n"
     ]
    }
   ],
   "source": [
    "# emnist network\n",
    "from models.nn import cntk_ConvNet as ConvNet\n",
    "# from models.nn import VGG_like as ConvNet\n",
    "\n",
    "hp_d = dict() # hyperparameters for a network\n",
    "mean = np.load('../refer/datasets-analysis-cntk/output/mean_emnist.npy')\n",
    "hp_d['image_mean'] = np.transpose(np.tile(mean,(H,W,1)),(2,0,1))\n",
    "\n",
    "net = ConvNet(test_set.__getitem__(0)[0].shape, len(anno_dict['classes']), **hp_d)\n",
    "net.logits.restore('/Data/checkpts/emnist/temp_model_fold_1_trainval_ratio_1.0.dnn')\n",
    "\n",
    "# emnist network: SANITY CHECK\n",
    "start_time = time.time()\n",
    "ys, y_preds, test_score, confusion_matrix = net.predict(test_set, **hp_d)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print('Test error rate: {}'.format(test_score))\n",
    "print('Total tack time(sec): {}'.format(total_time))\n",
    "print('Tact time per image(sec): {}'.format(total_time / len(test_list)))\n",
    "print('Confusion matrix: \\n{}'.format(confusion_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Hessian Vector Product w.r.t. Freezed Network\n",
    "\n",
    "Gradient, Hessian을 구할 때 weight의 범위를 한정지어서 inversed HVP를 구함.\n",
    "\n",
    "앞단 network를 freeze 시키는 데에는 두 가지 이유가 있음.\n",
    "\n",
    "1. Convexity\n",
    "    - Network가 깊어지면 깊어질 수록 convexity가 망가질 가능성이 있음.\n",
    "    - 때문에 CG, NCG, SE 방법론을 사용할 때 값이 발산하는 문제가 발생함.\n",
    "2. Computational Complexity\n",
    "    - Weight가 많을 수록 계산이 복잡해지고, vanishing gradient 등의 문제로 인해 precision loss가 발생할 가능성이 늘어남.\n",
    "    \n",
    "이를 해결하기 위해 가장 간단하고 직관적인 방법인 network freezing을 사용함.\n",
    "이는 feature extraction을 담당하는 앞부분 weight를 전부 고정하고, \n",
    "최종 layer의 weight만을 사용하여 convexity가 보장된 logistic regression문제로 바꾸는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testfile name:  train_n_76066.png\n",
      "ground truth label:  n\n",
      "network prediction:  n\n",
      "(Parameter('W', [], [96 x 47]), Parameter('b', [], [47]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwehee/anaconda3/envs/cntk/lib/python3.5/site-packages/ipykernel_launcher.py:30: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 ,  1.075157880783081 (sec) elapsed\n",
      "vector element-wise square:  0.0913993416143\n",
      "iteration: 1 ,  4.955273628234863 (sec) elapsed\n",
      "vector element-wise square:  0.0916222182896\n",
      "iteration: 2 ,  8.44317078590393 (sec) elapsed\n",
      "vector element-wise square:  0.0916998953871\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.200502\n",
      "         Iterations: 3\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 26\n",
      "get_influence_val takes 1.5060575008392334 sec\n",
      "CG_logreg takes 14.69578504562378 sec, and its max/min value [0.0017271030003903434, -0.0010314936068607494]\n"
     ]
    }
   ],
   "source": [
    "# DO THIS FOR SEVERAL EXAMPLES\n",
    "\n",
    "# vec v.s. freeze v.s. se\n",
    "\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict, rescale=False)\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict, rescale=False)\n",
    "\n",
    "save_dir = '/Data/result/influence'\n",
    "\n",
    "for idx_test in range(0, 1):\n",
    "    # Set a single test image\n",
    "\n",
    "    name_test = test_list[idx_test]\n",
    "    img_test, lb_test = test_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    #v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.squeeze(img_test))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['dense1'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "    # Calculate influence functions\n",
    "\n",
    "    # CG-FREEZE (1885 sec)\n",
    "    t1 = time.time()\n",
    "    ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':50})\n",
    "    IF_cg_logreg = get_influence_val(net, ihvp_cg_logreg, train_set)\n",
    "    print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "    np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "    #IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "    visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path=save_path+'/cg-frz')\n",
    "    \n",
    "#     # VECTOR-FREEZE (175 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_logreg = IF_val(net, v_logreg, train_set)\n",
    "#     print('V_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_logreg),min(IF_v_logreg)]))\n",
    "#     np.save(save_path+'/if_v_logreg.npy', IF_v_logreg)\n",
    "#     #IF_v_logreg = np.load(save_path+'/if_v_logreg.npy')\n",
    "#     visualize_topk_samples(IF_v_logreg, train_set, num_sample=5, save_path=save_path+'/vec-frz')\n",
    "\n",
    "#     # Vector-FULL (1688 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v = IF_val(net, v_test, train_set)\n",
    "#     print('V takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v),min(IF_v)]))\n",
    "#     np.save(save_path+'/if_v.npy', IF_v)\n",
    "#     #IF_v = np.load(save_path+'/if_v.npy')\n",
    "#     visualize_topk_samples(IF_v, train_set, num_sample=5, save_path=save_path+'/v')\n",
    "    \n",
    "#     # VECTOR-FREEZE-cosine-similarity (178 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_cos = IF_val(net, v_logreg, train_set, cosine=True)\n",
    "#     print('V_cos takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_cos),min(IF_v_cos)]))\n",
    "#     np.save(save_path+'/if_v_cos.npy', IF_v_cos)\n",
    "#     #IF_v_cos = np.load(save_path+'/if_v_cos.npy')\n",
    "#     visualize_topk_samples(IF_v_cos, train_set, num_sample=5, save_path=save_path+'/vec-cos')\n",
    "\n",
    "#     # CG-FULL (1epoch, more than 3 hours, did it stuck at line search as it happened in ncg?)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set,**{'damping':0.1, 'maxiter':100})\n",
    "#     IF_cg = IF_val(net, ihvp_cg, train_set)\n",
    "#     print('CG takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg),min(IF_cg)]))\n",
    "#     np.save(save_path+'/if_cg.npy', IF_cg)\n",
    "#     visualize_topk_samples(IF_cg, train_set, num_sample=5, save_path=save_path+'/cg')\n",
    "    \n",
    "#     # SE-FULL (? sec: diverge)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':1e5, 'damping':0.1, 'batch_size':50, 'recursion_depth':100})\n",
    "#     IF_se = IF_val(net, ihvp_se, train_set)\n",
    "#     print('SE takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se),min(IF_se)]))\n",
    "#     np.save(save_path+'/if_se.npy', IF_se)\n",
    "#     visualize_topk_samples(IF_se, train_set, num_sample=5, save_path=save_path+'/se')\n",
    "    \n",
    "#     # SE-FREEZE (1065 sec)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se_logreg = get_inverse_hvp_se(net, net.loss, v_logreg, train_set,**{'scale':1e3, 'damping':0.1, 'batch_size':50, 'tolerance':0, 'recursion_depth':65})\n",
    "#     IF_se_logreg = IF_val(net, ihvp_se_logreg, train_set)\n",
    "#     print('SE_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se_logreg),min(IF_se_logreg)]))\n",
    "#     np.save(save_path+'/if_se_logreg.npy', IF_se_logreg)\n",
    "#     #IF_se_logreg = np.load(save_path+'/if_se_logreg.npy')\n",
    "#     visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=save_path+'/se-frz')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO THIS FOR SEVERAL EXAMPLES\n",
    "\n",
    "# vec v.s. freeze v.s. se\n",
    "\n",
    "# restore trainval_list, test_list\n",
    "file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "\n",
    "trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "# FIXME\n",
    "# network trained with noisy dataset, influence value with noisy dataset\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, noisy_anno_dict) # noisy dataset\n",
    "save_dir = './compare/result_net_ns_if_ns' \n",
    "restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.3.dnn'\n",
    "\n",
    "# # network trained with non-noisy dataset, influence value with non-noisy dataset\n",
    "# train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "# save_dir = './compare/result_net_nn_if_nn'\n",
    "# restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn' # non-noisy network\n",
    "\n",
    "for idx_test in range(0, 30):\n",
    "    # Set a single test image\n",
    "\n",
    "    # # Re-sample a test instance\n",
    "    # test_list, _ = dataset.read_data_subset(root_dir, mode='validation1', sample_size=100)\n",
    "    # test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "    \n",
    "    # Restore weights\n",
    "    net.logits.restore(restore_dir)\n",
    "\n",
    "    params = net.logits.parameters\n",
    "\n",
    "    name_test = test_list[idx_test]\n",
    "    img_test, lb_test = test_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.squeeze(img_test))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['dense1'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "    # Calculate influence functions\n",
    "\n",
    "#     # CG-FREEZE (1885 sec)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':50})\n",
    "#     IF_cg_logreg = IF_val(net, ihvp_cg_logreg, train_set)\n",
    "#     print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "#     np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "#     #IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "#     visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path=save_path+'/cg-frz')\n",
    "    \n",
    "#     # VECTOR-FREEZE (175 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_logreg = IF_val(net, v_logreg, train_set)\n",
    "#     print('V_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_logreg),min(IF_v_logreg)]))\n",
    "#     np.save(save_path+'/if_v_logreg.npy', IF_v_logreg)\n",
    "#     #IF_v_logreg = np.load(save_path+'/if_v_logreg.npy')\n",
    "#     visualize_topk_samples(IF_v_logreg, train_set, num_sample=5, save_path=save_path+'/vec-frz')\n",
    "\n",
    "#     # Vector-FULL (1688 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v = IF_val(net, v_test, train_set)\n",
    "#     print('V takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v),min(IF_v)]))\n",
    "#     np.save(save_path+'/if_v.npy', IF_v)\n",
    "#     #IF_v = np.load(save_path+'/if_v.npy')\n",
    "#     visualize_topk_samples(IF_v, train_set, num_sample=5, save_path=save_path+'/v')\n",
    "    \n",
    "#     # VECTOR-FREEZE-cosine-similarity (178 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_cos = IF_val(net, v_logreg, train_set, cosine=True)\n",
    "#     print('V_cos takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_cos),min(IF_v_cos)]))\n",
    "#     np.save(save_path+'/if_v_cos.npy', IF_v_cos)\n",
    "#     #IF_v_cos = np.load(save_path+'/if_v_cos.npy')\n",
    "#     visualize_topk_samples(IF_v_cos, train_set, num_sample=5, save_path=save_path+'/vec-cos')\n",
    "\n",
    "#     # CG-FULL (1epoch, more than 3 hours, did it stuck at line search as it happened in ncg?)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set,**{'damping':0.1, 'maxiter':100})\n",
    "#     IF_cg = IF_val(net, ihvp_cg, train_set)\n",
    "#     print('CG takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg),min(IF_cg)]))\n",
    "#     np.save(save_path+'/if_cg.npy', IF_cg)\n",
    "#     visualize_topk_samples(IF_cg, train_set, num_sample=5, save_path=save_path+'/cg')\n",
    "    \n",
    "#     # SE-FULL (? sec: diverge)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':1e5, 'damping':0.1, 'batch_size':50, 'recursion_depth':100})\n",
    "#     IF_se = IF_val(net, ihvp_se, train_set)\n",
    "#     print('SE takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se),min(IF_se)]))\n",
    "#     np.save(save_path+'/if_se.npy', IF_se)\n",
    "#     visualize_topk_samples(IF_se, train_set, num_sample=5, save_path=save_path+'/se')\n",
    "    \n",
    "#     # SE-FREEZE (1065 sec)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se_logreg = get_inverse_hvp_se(net, net.loss, v_logreg, train_set,**{'scale':1e3, 'damping':0.1, 'batch_size':50, 'tolerance':0, 'recursion_depth':65})\n",
    "#     IF_se_logreg = IF_val(net, ihvp_se_logreg, train_set)\n",
    "#     print('SE_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se_logreg),min(IF_se_logreg)]))\n",
    "#     np.save(save_path+'/if_se_logreg.npy', IF_se_logreg)\n",
    "#     #IF_se_logreg = np.load(save_path+'/if_se_logreg.npy')\n",
    "#     visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=save_path+'/se-frz')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figures for ppts\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_idx(filename, filename_list):\n",
    "    i = 0\n",
    "    for fn in filename_list:\n",
    "        if fn == filename:\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "    return i\n",
    "\n",
    "filename = 'train_e_99502.png'\n",
    "fidx = find_idx(filename, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_sample = []\n",
    "for i in range(20):\n",
    "    img, lb = train_set[i]\n",
    "    fig_sample.append(img)\n",
    "#print(fig_sample[1].shape)\n",
    "fig_sample = np.concatenate(fig_sample, axis=1)\n",
    "show_image_from_data(fig_sample)\n",
    "scipy.misc.imsave('./images/image_samples_total.png', np.squeeze(fig_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize image samples (MASK)\n",
    "\n",
    "# FIXME #\n",
    "num_sample = 10\n",
    "filename = 'train_e_99502.png'\n",
    "file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "\n",
    "sample_idx = find_idx(filename, test_list)\n",
    "img_test, lb_test = test_set[0]\n",
    "show_image_from_data(img_test)\n",
    "mask = [anno_dict['images'][exmp]['class'][0] == np.argmax(lb_test) for exmp in trainval_list]\n",
    "mask_inv = [not e for e in mask]\n",
    "#len(mask_e)\n",
    "\n",
    "fig_sample = np.empty((1,28,0))\n",
    "\n",
    "# data with same label w.r.t test sample (set A)\n",
    "cnt = 0\n",
    "for i in range(len(trainval_list)):\n",
    "    if mask[i]:\n",
    "        img, lb = train_set[i]\n",
    "        fig_sample = np.concatenate((fig_sample, img), axis=2)\n",
    "        cnt += 1\n",
    "        if cnt == num_sample:\n",
    "            break\n",
    "\n",
    "print(fig_sample.shape)\n",
    "scipy.misc.imsave('./images/image_samples.png', np.squeeze(fig_sample))\n",
    "show_image_from_data(fig_sample)\n",
    "\n",
    "# data with the others (set B)\n",
    "fig_sample = np.empty((1,28,0))\n",
    "cnt = 0\n",
    "for i in range(len(trainval_list)):\n",
    "    if mask_inv[i]:\n",
    "        img, lb = train_set[i]\n",
    "        fig_sample = np.concatenate((fig_sample, img), axis=2)\n",
    "        cnt += 1\n",
    "        if cnt == num_sample:\n",
    "            break\n",
    "\n",
    "print(fig_sample.shape)\n",
    "scipy.misc.imsave('./images/image_samples2.png', np.squeeze(fig_sample))\n",
    "show_image_from_data(fig_sample)\n",
    "\n",
    "# data somewhat similar to each other (A n B)\n",
    "IF_cg_logreg = np.load(file_dir+'/if_cg_logreg.npy')\n",
    "visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path='./images/all') # show all\n",
    "visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, mask=mask, save_path='./images/masked') # masked\n",
    "visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, mask=mask_inv, save_path='./images/masked_inv') # masked\n",
    "\n",
    "# idx_sort_mask = list(filter(lambda idx: mask[idx], np.argsort(IF_cg_logreg)))\n",
    "\n",
    "# for idx in idx_sort_mask:\n",
    "#     show_image_from_data(train_set[idx][0])\n",
    "#     print(IF_cg_logreg[idx])\n",
    "\n",
    "    #show top bot inf neg\n",
    "#     break\n",
    "# for fige in mask_e[0:5]:\n",
    "#     idx = find_idx(mask_e, trainval_list)\n",
    "#     print(fige,idx)\n",
    "#     show_image_from_data(train_set[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# axis change\n",
    "\n",
    "# import glob\n",
    "\n",
    "# file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "# restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn' # non-noisy network\n",
    "# net.logits.restore(restore_dir)\n",
    "\n",
    "# trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "# train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "# test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "# test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "# for dr in glob.glob('compare/result_net_nn_if_nn/*'):\n",
    "#     dr_se = dr+'/if_se_logreg.npy'\n",
    "#     if os.path.isfile(dr_se):\n",
    "#         print(dr_se)\n",
    "#         IF_se_logreg = np.load(dr_se)\n",
    "#         visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=dr+'/se-frz')\n",
    "        \n",
    "import glob\n",
    "\n",
    "file_dir = './compare/result_net_ns_if_ns/train_e_99502'\n",
    "restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.3.dnn' # noisy network\n",
    "net.logits.restore(restore_dir)\n",
    "\n",
    "trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, noisy_anno_dict) # noisy dataset\n",
    "\n",
    "test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "for dr in glob.glob('compare/result_net_ns_if_ns/*'):\n",
    "    dr_se = dr+'/if_se_logreg.npy'\n",
    "    if os.path.isfile(dr_se):\n",
    "        print(dr_se)\n",
    "        IF_se_logreg = np.load(dr_se)\n",
    "        visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=dr+'/se-frz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate IF measure for each samples of each classes\n",
    "\n",
    "# WITH SEVERAL METHODOLOGIES\n",
    "\n",
    "temp_list, _ = dataset.read_data_subset(root_dir, mode='validation1')\n",
    "\n",
    "print('num_of_samples',len(temp_list))\n",
    "\n",
    "# restore trainval_list, test_list\n",
    "#file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "file_dir = './sample/result_net_nn_if_nn/train_B_69574'\n",
    "\n",
    "trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "\n",
    "dic = {ks: [] for ks in anno_dict['classes'].values()}\n",
    "\n",
    "for tl in temp_list:\n",
    "    dic[tl.split('_')[1]].append(tl)\n",
    "    \n",
    "#sample_list = [dic[ks][0] for ks in dic.keys()]\n",
    "sample_list = list(np.load('./sample/result_net_nn_if_nn/train_B_69574/sample_list.npy'))\n",
    "sample_set = dataset.LazyDataset(root_dir, sample_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "    \n",
    "print(len(sample_list), sample_list)\n",
    "\n",
    "# # network trained with non-noisy dataset, influence value with non-noisy dataset\n",
    "# save_dir = './sample/result_net_nn_if_nn'\n",
    "# restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn' # non-noisy network\n",
    "# train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "# network trained with noisy dataset, influence value with noisy dataset\n",
    "save_dir = './sample/result_net_ns_if_ns'\n",
    "restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.3.dnn' # noisy network\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, noisy_anno_dict) # noisy dataset\n",
    "\n",
    "\n",
    "for idx_test in range(47):\n",
    "    # Set a single test image\n",
    "\n",
    "    # Restore weights\n",
    "    net.logits.restore(restore_dir)\n",
    "\n",
    "    params = net.logits.parameters\n",
    "\n",
    "    name_test = sample_list[idx_test]\n",
    "    img_test, lb_test = sample_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.squeeze(img_test))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "    np.save(save_path+'/temp_list', temp_list)\n",
    "    np.save(save_path+'/sample_list', sample_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['dense1'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "#     # Calculate influence functions\n",
    "\n",
    "#     # CG-FREEZE (1885 sec)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':50})\n",
    "#     IF_cg_logreg = IF_val(net, ihvp_cg_logreg, train_set)\n",
    "#     print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "#     np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "#     #IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "#     visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path=save_path+'/cg-frz')\n",
    "    \n",
    "#     # VECTOR-FREEZE (175 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_logreg = IF_val(net, v_logreg, train_set)\n",
    "#     print('V_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_logreg),min(IF_v_logreg)]))\n",
    "#     np.save(save_path+'/if_v_logreg.npy', IF_v_logreg)\n",
    "#     #IF_v_logreg = np.load(save_path+'/if_v_logreg.npy')\n",
    "#     visualize_topk_samples(IF_v_logreg, train_set, num_sample=5, save_path=save_path+'/vec-frz')\n",
    "\n",
    "#     # Vector-FULL (1688 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v = IF_val(net, v_test, train_set)\n",
    "#     print('V takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v),min(IF_v)]))\n",
    "#     np.save(save_path+'/if_v.npy', IF_v)\n",
    "#     #IF_v = np.load(save_path+'/if_v.npy')\n",
    "#     visualize_topk_samples(IF_v, train_set, num_sample=5, save_path=save_path+'/v')\n",
    "    \n",
    "#     # VECTOR-FREEZE-cosine-similarity (178 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_cos = IF_val(net, v_logreg, train_set, cosine=True)\n",
    "#     print('V_cos takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_cos),min(IF_v_cos)]))\n",
    "#     np.save(save_path+'/if_v_cos.npy', IF_v_cos)\n",
    "#     #IF_v_cos = np.load(save_path+'/if_v_cos.npy')\n",
    "#     visualize_topk_samples(IF_v_cos, train_set, num_sample=5, save_path=save_path+'/vec-cos')\n",
    "\n",
    "#     # CG-FULL (1epoch, more than 3 hours, did it stuck at line search as it happened in ncg?)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set,**{'damping':0.1, 'maxiter':100})\n",
    "#     IF_cg = IF_val(net, ihvp_cg, train_set)\n",
    "#     print('CG takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg),min(IF_cg)]))\n",
    "#     np.save(save_path+'/if_cg.npy', IF_cg)\n",
    "#     visualize_topk_samples(IF_cg, train_set, num_sample=5, save_path=save_path+'/cg')\n",
    "    \n",
    "#     # SE-FULL (? sec: diverge)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':1e5, 'damping':0.1, 'batch_size':50, 'recursion_depth':100})\n",
    "#     IF_se = IF_val(net, ihvp_se, train_set)\n",
    "#     print('SE takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se),min(IF_se)]))\n",
    "#     np.save(save_path+'/if_se.npy', IF_se)\n",
    "#     visualize_topk_samples(IF_se, train_set, num_sample=5, save_path=save_path+'/se')\n",
    "    \n",
    "    # SE-FREEZE (1065 sec)\n",
    "    t1 = time.time()\n",
    "    ihvp_se_logreg = get_inverse_hvp_se(net, net.loss, v_logreg, train_set,**{'scale':1e3, 'damping':0.1, 'batch_size':50, 'tolerance':0, 'recursion_depth':65})\n",
    "    IF_se_logreg = IF_val(net, ihvp_se_logreg, train_set)\n",
    "    print('SE_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se_logreg),min(IF_se_logreg)]))\n",
    "    np.save(save_path+'/if_se_logreg.npy', IF_se_logreg)\n",
    "    #IF_se_logreg = np.load(save_path+'/if_se_logreg.npy')\n",
    "    visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=save_path+'/se-frz')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_list(ls):\n",
    "    # normalize elements in the list. \n",
    "    # the distribution of each element follows normal distribution\n",
    "    # i.e. (x_i-mean)/sigma\n",
    "    \n",
    "    m = np.mean(ls)\n",
    "    v = np.sqrt(np.var(ls))\n",
    "    \n",
    "    return np.array([(e-m)/v for e in ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summation over several examples and visualize samples\n",
    "\n",
    "import glob\n",
    "\n",
    "# file_dir = './sample/result_net_nn_if_nn/train_N_43201'\n",
    "# restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn' # non-noisy network\n",
    "# net.logits.restore(restore_dir)\n",
    "\n",
    "# trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "# train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "# test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "# test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "file_dir = './sample/result_net_ns_if_ns/train_N_43201'\n",
    "restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.3.dnn' # non-noisy network\n",
    "net.logits.restore(restore_dir)\n",
    "\n",
    "trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, noisy_anno_dict) # non-noisy dataset\n",
    "\n",
    "test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "IF_batch = []\n",
    "\n",
    "for dr in glob.glob('sample/result_net_ns_if_ns/*'):\n",
    "    #dr_if = dr+'/if_v_logreg.npy'\n",
    "    dr_if = dr+'/if_se_logreg.npy'\n",
    "    if os.path.isfile(dr_if):\n",
    "        print(dr_if)\n",
    "        IF = np.load(dr_if)\n",
    "        # normalize?\n",
    "        #IF = normalize_list(IF)\n",
    "        IF_batch.append(IF)\n",
    "\n",
    "IF_batch = np.mean(IF_batch, axis=0)\n",
    "visualize_topk_samples(IF_batch, train_set, num_sample=20, save_path='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate IF measure w.r.t. fixed length of dataset\n",
    "\n",
    "# WITH SEVERAL METHODOLOGIES\n",
    "\n",
    "temp_list, _ = dataset.read_data_subset(root_dir, mode='validation1')\n",
    "\n",
    "print('num_of_samples',len(temp_list))\n",
    "\n",
    "# restore trainval_list, test_list\n",
    "file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "\n",
    "trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "dic = {ks: [] for ks in anno_dict['classes'].values()}\n",
    "\n",
    "for tl in temp_list:\n",
    "    dic[tl.split('_')[1]].append(tl)\n",
    "    \n",
    "sample_list = [dic[ks][0] for ks in dic.keys()]\n",
    "sample_set = dataset.LazyDataset(root_dir, sample_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "    \n",
    "print(len(sample_list), sample_list)\n",
    "\n",
    "# 저장경로 변경 (compare 말고)\n",
    "\n",
    "# network trained with non-noisy dataset, influence value with non-noisy dataset\n",
    "save_dir = './sample/result_net_nn_if_nn'\n",
    "restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn' # non-noisy network\n",
    "\n",
    "for idx_test in range(47):\n",
    "    # Set a single test image\n",
    "\n",
    "    # Restore weights\n",
    "    net.logits.restore(restore_dir)\n",
    "\n",
    "    params = net.logits.parameters\n",
    "\n",
    "    name_test = sample_list[idx_test]\n",
    "    img_test, lb_test = sample_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.squeeze(img_test))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "    np.save(save_path+'/temp_list', temp_list)\n",
    "    np.save(save_path+'/sample_list', sample_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['dense1'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "#     # Calculate influence functions\n",
    "\n",
    "#     # CG-FREEZE (1885 sec)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':50})\n",
    "#     IF_cg_logreg = IF_val(net, ihvp_cg_logreg, train_set)\n",
    "#     print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "#     np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "#     #IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "#     visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path=save_path+'/cg-frz')\n",
    "    \n",
    "#     # VECTOR-FREEZE (175 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_logreg = IF_val(net, v_logreg, train_set)\n",
    "#     print('V_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_logreg),min(IF_v_logreg)]))\n",
    "#     np.save(save_path+'/if_v_logreg.npy', IF_v_logreg)\n",
    "#     #IF_v_logreg = np.load(save_path+'/if_v_logreg.npy')\n",
    "#     visualize_topk_samples(IF_v_logreg, train_set, num_sample=5, save_path=save_path+'/vec-frz')\n",
    "\n",
    "#     # Vector-FULL (1688 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v = IF_val(net, v_test, train_set)\n",
    "#     print('V takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v),min(IF_v)]))\n",
    "#     np.save(save_path+'/if_v.npy', IF_v)\n",
    "#     #IF_v = np.load(save_path+'/if_v.npy')\n",
    "#     visualize_topk_samples(IF_v, train_set, num_sample=5, save_path=save_path+'/v')\n",
    "    \n",
    "#     # VECTOR-FREEZE-cosine-similarity (178 sec)\n",
    "#     t1 = time.time()\n",
    "#     IF_v_cos = IF_val(net, v_logreg, train_set, cosine=True)\n",
    "#     print('V_cos takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_cos),min(IF_v_cos)]))\n",
    "#     np.save(save_path+'/if_v_cos.npy', IF_v_cos)\n",
    "#     #IF_v_cos = np.load(save_path+'/if_v_cos.npy')\n",
    "#     visualize_topk_samples(IF_v_cos, train_set, num_sample=5, save_path=save_path+'/vec-cos')\n",
    "\n",
    "#     # CG-FULL (1epoch, more than 3 hours, did it stuck at line search as it happened in ncg?)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set,**{'damping':0.1, 'maxiter':100})\n",
    "#     IF_cg = IF_val(net, ihvp_cg, train_set)\n",
    "#     print('CG takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg),min(IF_cg)]))\n",
    "#     np.save(save_path+'/if_cg.npy', IF_cg)\n",
    "#     visualize_topk_samples(IF_cg, train_set, num_sample=5, save_path=save_path+'/cg')\n",
    "    \n",
    "#     # SE-FULL (? sec: diverge)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':1e5, 'damping':0.1, 'batch_size':50, 'recursion_depth':100})\n",
    "#     IF_se = IF_val(net, ihvp_se, train_set)\n",
    "#     print('SE takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se),min(IF_se)]))\n",
    "#     np.save(save_path+'/if_se.npy', IF_se)\n",
    "#     visualize_topk_samples(IF_se, train_set, num_sample=5, save_path=save_path+'/se')\n",
    "    \n",
    "    # SE-FREEZE (1065 sec)\n",
    "    t1 = time.time()\n",
    "    ihvp_se_logreg = get_inverse_hvp_se(net, net.loss, v_logreg, train_set,**{'scale':1e3, 'damping':0.1, 'batch_size':50, 'tolerance':0, 'recursion_depth':65})\n",
    "    IF_se_logreg = IF_val(net, ihvp_se_logreg, train_set)\n",
    "    print('SE_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se_logreg),min(IF_se_logreg)]))\n",
    "    np.save(save_path+'/if_se_logreg.npy', IF_se_logreg)\n",
    "    #IF_se_logreg = np.load(save_path+'/if_se_logreg.npy')\n",
    "    visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=save_path+'/se-frz')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "border 2\n",
    "======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare value between two labels\n",
    "\n",
    "IF_f = np.load('./sample/result_net_nn_if_nn/train_f_952/if_se_logreg.npy')\n",
    "IF_F = np.load('./sample/result_net_nn_if_nn/train_F_41402/if_se_logreg.npy')\n",
    "\n",
    "idx = np.argmax(IF_f)\n",
    "print(idx)\n",
    "img_temp, _ = train_set[idx]\n",
    "show_image_from_data(img_temp)\n",
    "print(IF_f[idx], IF_F[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename folder (test_list -> sample_list)\n",
    "dr = './sample/result_net_nn_if_nn/'\n",
    "\n",
    "for i in range(1,47):\n",
    "    tn = test_list[i].split('.')[0]; sn = sample_list[i].split('.')[0]\n",
    "    print('from {} to {}'.format(tn, sn))\n",
    "    tt = list(filter(lambda x: 'reference' in x, glob.glob(dr+tn+'/*')))[0]\n",
    "    print(tt.split('_')[-3])\n",
    "    os.rename(dr+tn, dr+sn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rename result images (advantageous, true_x1_noisy_x2 -> true_x2_noisy_x1)\n",
    "dr = './sample/result_net_ns_if_ns/'\n",
    "\n",
    "DOOOOOOOOOOOOOOOOOTHISSSSSSSSSSSSSSSSSSSSSSSSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RELABELING\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import dataset28 as dataset\n",
    "from models.nn import VGG as ConvNet\n",
    "from learning.evaluators import ErrorRateEvaluator as Evaluator\n",
    "\n",
    "import time\n",
    "\n",
    "def review(ratios, method):\n",
    "    # ratios: the list of ratio which is the proportion of the data considered to be reviewed.  \n",
    "    # reviewing is done by oracle. the label may or may not be changed. \n",
    "    #(i.e. if a single reviewed data has correct label, the label won't be changed, and vice versa)\n",
    "    # method: the methodology of selecting data torch be reviewed. \n",
    "    # this can be 'random', 'influence', loss', 'entropy'\n",
    "    t1 = time.time()\n",
    "\n",
    "    # FIXME\n",
    "    anno_dir = '/Data/emnist/balanced/original/annotation/'\n",
    "    root_dir = '/Data/emnist/balanced/original/'\n",
    "    #checkpt_dir = '/Data/github/interview/save/dropout_0.5_noaugmentation/model_fold_1_trainval_ratio_0.3.dnn'\n",
    "    #checkpt_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn'\n",
    "    checkpt_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.3.dnn'\n",
    "\n",
    "    with open(anno_dir + 'annotation1.json', 'r') as fid:\n",
    "        annotation = json.load(fid)\n",
    "\n",
    "    with open(anno_dir + 'annotation1_wp_0.3.json', 'r') as fid:\n",
    "        noisy_annotation = json.load(fid)\n",
    "\n",
    "    #image_list = list(noisy_annotation['images'].keys())\n",
    "    image_list = trainval_list\n",
    "    num_image = len(image_list)\n",
    "\n",
    "    ## sorting\n",
    "    if method == 'random':\n",
    "        # random policy\n",
    "        image_list_random = image_list[:]\n",
    "        random.shuffle(image_list_random)\n",
    "        review_list = image_list_random\n",
    "        #review_list_random = np.random.choice(image_list, int(num_image * ratio), replace=False)\n",
    "        \n",
    "    elif method == 'influence':\n",
    "        # influence function\n",
    "        save_path = './sample/result_net_ns_if_ns/train_K_97272'\n",
    "        IF_measure = np.load(save_path+'/if_se_logreg.npy')\n",
    "        argsort_abs = np.argsort(np.abs(IF_measure))[::-1]\n",
    "        #review_list = image_list[argsort_abs]\n",
    "        review_list = [image_list[idx] for idx in argsort_abs]\n",
    "#         noisy_list = [noisy_annotation['images'][fname]['class'] for fname in review_list]\n",
    "#         print(review_list[0:int(num_image * ratios[0])])\n",
    "#         print(noisy_list[0:int(num_image * ratios[0])])\n",
    "\n",
    "    elif method == 'influence-sum':\n",
    "        # summation of influence function among several samples\n",
    "        save_path = glob.glob('./sample/result_net_ns_if_ns/*')\n",
    "        IF_measures = []\n",
    "        for pth in save_path:\n",
    "            IF_measure = np.load(pth+'/if_se_logreg.npy')\n",
    "            IF_measures.append(np.abs(IF_measure))\n",
    "        IF_measures = np.mean(IF_measures, axis=0)\n",
    "        argsort_abs = np.argsort(IF_measures)[::-1]\n",
    "        review_list = [image_list[idx] for idx in argsort_abs]\n",
    "        \n",
    "    else:\n",
    "        # loss\n",
    "        image_set = dataset.LazyDataset(root_dir, image_list, noisy_annotation)\n",
    "        model = ConvNet(image_set.__getitem__(0)[0].shape, len(annotation['classes']))\n",
    "        model.logits.restore(checkpt_dir)\n",
    "        evaluator = Evaluator()\n",
    "        \n",
    "        # extract loss, entropy\n",
    "        t1_measure = time.time()\n",
    "        loss, entropy = network_based_measure(model, image_set)\n",
    "        t2_measure = time.time()\n",
    "        print('measure extraction takes {}'.format(t2_measure-t1_measure))\n",
    "        # check data // filename[0] and __getitem__[0] and dataloader first instance\n",
    "        # -> all of them are same. in other word, we can use an index information\n",
    "        \n",
    "        if method == 'loss':\n",
    "            # loss ascending policy\n",
    "            idx_loss = np.argsort(loss)[::-1]\n",
    "            image_list_loss = [image_list[i] for i in idx_loss]\n",
    "            review_list = image_list_loss\n",
    "        \n",
    "        elif method == 'entropy':\n",
    "            # entropy ascending policy\n",
    "            idx_entropy = np.argsort(entropy)[::-1]\n",
    "            image_list_entropy = [image_list[i] for i in idx_entropy]\n",
    "            review_list = image_list_entropy\n",
    "\n",
    "    ## correcting\n",
    "    corrected_list = []\n",
    "    for ratio in ratios:\n",
    "        print(ratio)\n",
    "        num_corrected = 0\n",
    "        review_list_ratio = review_list[0:int(num_image * ratio)]\n",
    "        print(len(review_list_ratio))\n",
    "        for fname in review_list_ratio:\n",
    "            correct_class = annotation['images'][fname]['class']\n",
    "            noisy_class = noisy_annotation['images'][fname]['class']\n",
    "            if noisy_class != correct_class:\n",
    "                num_corrected += 1\n",
    "        #corrected_list.append(num_corrected/int(0.3*len(image_list)))\n",
    "        corrected_list.append(num_corrected)\n",
    "\n",
    "    return corrected_list\n",
    "    #return [cr/int(0.3*len(image_list)) for cr in corrected_list]\n",
    "\n",
    "def network_based_measure(model, data_set):\n",
    "    # return loss and entropy\n",
    "    batch_size = 256\n",
    "    num_workers = 6\n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    num_classes = len(data_set.anno_dict['classes'])\n",
    "\n",
    "    loss = np.empty(0)\n",
    "    entropy = np.empty(0)\n",
    "    \n",
    "    # prediction in batchwise\n",
    "    for X, y in dataloader:\n",
    "        X = X.numpy(); y = y.numpy()\n",
    "        y_pred = model.pred.eval({model.X: X})\n",
    "        loss_batch = -np.log(np.sum(y_pred * y, axis=1))\n",
    "        entropy_batch = -np.sum(y_pred * np.log(y_pred), axis=1)\n",
    "        loss = np.concatenate((loss,loss_batch), axis=0)\n",
    "        entropy = np.concatenate((entropy, entropy_batch))\n",
    "\n",
    "    return loss, entropy\n",
    "\n",
    "# main code\n",
    "\n",
    "#x = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "#x = [0.001]\n",
    "#x = [0.1, 0.5, 0.9]\n",
    "x = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "rnd = review(x, 'random')\n",
    "ls = review(x, 'loss')\n",
    "etp = review(x, 'entropy')\n",
    "inf1 = review(x, 'influence')\n",
    "inf2 = review(x, 'influence-sum')\n",
    "# vector\n",
    "# vector frz\n",
    "# vector cos\n",
    "\n",
    "print(rnd)\n",
    "print(ls)\n",
    "print(etp)\n",
    "print(inf1)\n",
    "print(inf2)\n",
    "\n",
    "# draw a graph\n",
    "fig, ax = plt.subplots(1,1, figsize=(9,9))\n",
    "_ = ax.plot(x, rnd, color='b', label='random')\n",
    "_ = ax.plot(x, ls, color='g', label='loss')\n",
    "_ = ax.plot(x, etp, color='r', label='entropy') \n",
    "_ = ax.plot(x, inf1, color='y', label='influence') \n",
    "_ = ax.plot(x, inf2, color='c', label='influence-sum') \n",
    "_ = ax.set_title('Recovery results')\n",
    "_ = ax.set_ylabel('Ratio of corrected labeled: $Num_{corrected}/Num_{mislabeled}$')\n",
    "_ = ax.set_xlabel('Ratio of reviewed data: $Num_{reviewed}/Num_{total}$')\n",
    "_ = ax.set_xticks(x)\n",
    "_ = plt.legend()\n",
    "plt.savefig('./images/recovery_results.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wasted!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DO THIS FOR SEVERAL EXAMPLES\n",
    "\n",
    "# FIXME\n",
    "# # network trained with noisy dataset, influence value with noisy dataset\n",
    "# train_set = dataset.LazyDataset(root_dir, trainval_list, noisy_anno_dict) # noisy dataset\n",
    "# save_dir = './result_net_ns_if_ns' \n",
    "# restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.3.dnn'\n",
    "\n",
    "# network trained with non-noisy dataset, influence value with non-noisy dataset\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "save_dir = './result_net_nn_if_nn'\n",
    "restore_dir = '/Data/checkpts/emnist/model_fold_1_trainval_ratio_0.0.dnn' # non-noisy network\n",
    "\n",
    "for idx_test in range(13, 14):\n",
    "    # Set a single test image\n",
    "\n",
    "    # # Re-sample a test instance\n",
    "    # test_list, _ = dataset.read_data_subset(root_dir, mode='validation1', sample_size=100)\n",
    "    # test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "    \n",
    "    # Restore weights\n",
    "    net.logits.restore(restore_dir)\n",
    "\n",
    "    params = net.logits.parameters\n",
    "\n",
    "    name_test = test_list[idx_test]\n",
    "    img_test, lb_test = test_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.squeeze(img_test))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['dense1'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "    # Calculate influence functions\n",
    "\n",
    "    # the solution which is converged properly can be found within 30 iterations, otherwise does not converge\n",
    "    t1 = time.time()\n",
    "    ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':30})\n",
    "    IF_cg_logreg = IF_val(net, ihvp_cg_logreg, train_set)\n",
    "    print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "\n",
    "    np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "\n",
    "    # otherwise, load\n",
    "    IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "\n",
    "    # t1 = time.time()\n",
    "    # ihvp_ncg_logreg = get_inverse_hvp_ncg(net, net.loss, v_logreg, train_set,**{'damping':0.1, 'maxiter':3})\n",
    "    # IF_ncg_logreg = IF_val(net, ihvp_ncg_logreg, train_set)\n",
    "    # print('NCG_logreg takes {} sec, and its value {}'.format(time.time()-t1, IF_ncg_logreg))\n",
    "   \n",
    "    visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tsne\n",
    "tsne.py 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Retraining\n",
    "bootstrapping 참고\n",
    "\n",
    "그리고 SE도 돌려볼 것.\n",
    "그리고 여러 z_test의 if에 대해서 summation을 한 후 sorting 보기\n",
    "(그래서 전체적으로 성능을 나쁘게 하는 녀석이 있는지 찾기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[trainval_list]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
