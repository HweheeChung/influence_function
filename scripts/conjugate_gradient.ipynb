{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연구자: 정회희 (2018/04/24)\n",
    "\n",
    "해당 코드는 Understanding Black-box Predictions via Influence Functions 논문의 Conjugate Gradient를 구현한 코드이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretical Backgrounds\n",
    "-----------------------\n",
    "\n",
    "#### Preliminaries\n",
    "\n",
    "- Krylov Subspace\n",
    "- Caley-Hamilton Theorem\n",
    "\n",
    "#### Settings\n",
    "\n",
    "Given, \n",
    "\n",
    "\\begin{align*}\n",
    "& \\hat{\\theta} \\in \\mathbb{R}^p \\\\\n",
    "& v = \\nabla L(z_{test}, \\hat{\\theta}) \\in \\mathbb{R}^p \\\\\n",
    "& H = \\frac{1}{n}\\sum_{i=0}^{n}{\\nabla^2 L(z_i,\\hat{\\theta})} \\in \\mathbb{R}^{p^2}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Our goal is to find $x^* \\rightarrow H^{-1}v$\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "> $x:= 0, r:= v, \\rho_0 := \\lvert\\lvert r \\rvert\\rvert ^2$\n",
    "\n",
    "> for $ k=1,\\ldots , N_{max} $\n",
    "\n",
    ">> if $\\sqrt{\\rho_{k-1}}\\le \\epsilon \\lvert\\lvert v \\rvert\\rvert$ : break\n",
    "\n",
    ">> if $k=1$ then $p := r$; else $p:=r+\\left( \\frac{\\rho_{k-1}}{\\rho_{k-2}} \\right) p$\n",
    "\n",
    ">> $w := Hp$\n",
    "\n",
    ">> $\\alpha := \\frac{\\rho_{k}}{p^Tw}$\n",
    "\n",
    ">> $x:=x+\\alpha p$\n",
    "\n",
    ">> $r := r-\\alpha w$\n",
    "\n",
    ">> $\\rho_{k}:=\\lvert\\lvert r \\rvert\\rvert ^2$\n",
    "\n",
    "Remark) \n",
    "- $w := Hp$ 진행할 때 HVP를 사용하면 $O(np)$만큼 computational complexity 듬.\n",
    "- 실제 code에서는 모든 training dataset에 대해서 HVP를 하려면 한번에 모든 training set에 대해서 feed_dict해줘야하기 때문에 메모리 문제로 HVP_minibatch를 구현함. (밑 코드 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementations\n",
    "---------------\n",
    "\n",
    "실제 code에서는 모든 training dataset에 대해서 HVP를 하려면 한번에 모든 training set에 대해서 feed_dict해줘야하기 때문에 메모리 문제로 HVP_minibatch를 구현함. (밑 코드 참고)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def minibatch_hessian_vector_val(self, v):\n",
    "\n",
    "        num_examples = self.num_train_examples\n",
    "        if self.mini_batch == True:\n",
    "            batch_size = 100\n",
    "            assert num_examples % batch_size == 0\n",
    "        else:\n",
    "            batch_size = self.num_train_examples\n",
    "\n",
    "        num_iter = int(num_examples / batch_size)\n",
    "\n",
    "        self.reset_datasets()\n",
    "        hessian_vector_val = None\n",
    "        for i in xrange(num_iter):\n",
    "            feed_dict = self.fill_feed_dict_with_batch(self.data_sets.train, batch_size=batch_size)\n",
    "            # Can optimize this\n",
    "            feed_dict = self.update_feed_dict_with_v_placeholder(feed_dict, v)\n",
    "            hessian_vector_val_temp = self.sess.run(self.hessian_vector, feed_dict=feed_dict)\n",
    "            if hessian_vector_val is None:\n",
    "                hessian_vector_val = [b / float(num_iter) for b in hessian_vector_val_temp]\n",
    "            else:\n",
    "                hessian_vector_val = [a + (b / float(num_iter)) for (a,b) in zip(hessian_vector_val, hessian_vector_val_temp)]\n",
    "            \n",
    "        hessian_vector_val = [a + self.damping * b for (a,b) in zip(hessian_vector_val, v)]\n",
    "\n",
    "        return hessian_vector_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 conjugate gradient method 방법은 $x^* = H^{-1}v$를 구하고 싶은데, $H$ 차원이 너무 커서 직접적으로 matrix를 계산하고, 역행렬을 얻는 것이 힘들 때 \n",
    "\n",
    "\\begin{equation*}\n",
    "x^* = \\min_{x}{\\frac{1}{2}x^T H x - v^T x}\n",
    "\\end{equation*}\n",
    "\n",
    "를 풀어서 얻는 방법.\n",
    "\n",
    "때문에 quadratic programming 형태여야만 이론적 근거를 충족시킴.\n",
    "(refer: Krylov space)\n",
    "\n",
    "scipy package에서 제공하는 fmin_ncg는 임의의 convex 함수를 풀 때 conjugate gradient의 iteration 방법을 써서 풀겠다는 것. 주어진 objective function이 convex function 이라면 2nd order Taylor approximation을 할 수 있을 것이고 approximation을 하고 나면 정확하지는 않겠지만 어느정도 비슷하고 convergent한 해를 얻을 수 있다는 것. 따라서 fmin_ncg는 conjugate gradient의 iteration 방법으로 optimization 문제를 풀겠다는 것. \n",
    "\n",
    "이렇게 풀 경우에는 Hessian을 직접적으로 구하지 않고 (HVP만 사용해서) Newton's method 방식으로 optimize가 될 것.\n",
    "\n",
    "cf) 좀 더 general하게 설계되어있다 정도만 알면 됨. 어차피 우리는 objective function에 quadratic function을 넣을거고 그러면 전혀 문제 없음.\n",
    "\n",
    "Scipy package의 fmin conjugate gradient는 HVP 함수를 explicit하게 제공하면 더 정확하게 풀 수 있음. 따라서 위의 HVP를 사용할 예정.\n",
    "\n",
    "사실 HVP를 default로 해도 알아서 계산해주기 때문에 상관없지만, 우리의 경우 network parameter가 복잡하기 때문에 (layer별로 list화 되어있기 때문에), 그리고 한번에 많은 data point를 feed_dict하기 때문에 추가적인 작업이 필요함. 때문에 여기서는 명확하게 HVP를 제공함. \n",
    "\n",
    "아래 코드는 scipy.optimize.fmin_ncg를 사용하여 conjugate gradient를 하는 것.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    from scipy.optimize import fmin_ncg\n",
    "    \n",
    "    def get_inverse_hvp_cg(self, v, verbose):\n",
    "        fmin_loss_fn = self.get_fmin_loss_fn(v)\n",
    "        fmin_grad_fn = self.get_fmin_grad_fn(v)\n",
    "        cg_callback = self.get_cg_callback(v, verbose)\n",
    "\n",
    "        fmin_results = fmin_ncg(\n",
    "            f=fmin_loss_fn,\n",
    "            x0=np.concatenate(v),\n",
    "            fprime=fmin_grad_fn,\n",
    "            fhess_p=self.get_fmin_hvp,\n",
    "            callback=cg_callback,\n",
    "            avextol=1e-8,\n",
    "            maxiter=100) \n",
    "\n",
    "        return self.vec_to_list(fmin_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fmin_ncg의 input으로는\n",
    "\n",
    "- f: objective function to be minimized ~ $f(x)=\\frac{1}{2}x^T H x - v^T x$ where $H\\triangleq \\frac{1}{n}\\sum_{i=0}^{n}{\\nabla^2 L(z_i,x)}$\n",
    "- x0: initial guess\n",
    "- fprime: gradient of f ~ $f(x)=H x - v$\n",
    "- fhess_p: function to compute the Hessian matrix of f ~ $f(x,p)=H p$\n",
    "- callback: an optional user-supplied function which is called after each iteration\n",
    "- avextol: hyperparameter epsilon \n",
    "- maxiter: maximum number of iterations to perform\n",
    "\n",
    "이 있음.\n",
    "\n",
    "위에서 설명했듯 f, fprim, fhess_p는 특정 값이 아니고 input을 받으면 output을 내보내는 함수여야 함.\n",
    "\n",
    "code에서는 다음과 같이 정의함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_fmin_loss_fn(self, v):\n",
    "\n",
    "        def get_fmin_loss(x):\n",
    "            hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(x))\n",
    "\n",
    "            return 0.5 * np.dot(np.concatenate(hessian_vector_val), x) - np.dot(np.concatenate(v), x)\n",
    "        return get_fmin_loss\n",
    "\n",
    "\n",
    "    def get_fmin_grad_fn(self, v):\n",
    "        def get_fmin_grad(x):\n",
    "            hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(x))\n",
    "            \n",
    "            return np.concatenate(hessian_vector_val) - np.concatenate(v)\n",
    "        return get_fmin_grad\n",
    "\n",
    "\n",
    "    def get_fmin_hvp(self, x, p):\n",
    "        hessian_vector_val = self.minibatch_hessian_vector_val(self.vec_to_list(p))\n",
    "\n",
    "        return np.concatenate(hessian_vector_val)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
