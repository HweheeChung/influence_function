{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of IHVP w.r.t. Freezed Network\n",
    "\n",
    "Gradient, Hessian을 구할 때 weight의 범위를 한정지어서 inversed HVP를 구함.\n",
    "\n",
    "앞단 network를 freeze 시키는 데에는 두 가지 이유가 있음.\n",
    "\n",
    "1. Convexity\n",
    "    - Network가 깊어지면 깊어질 수록 convexity가 망가질 가능성이 있음.\n",
    "    - 때문에 CG, NCG, SE 방법론을 사용할 때 발산하는 등 여러 문제가 발생함.\n",
    "    - 이를 해결하기 위한 극단적인 예시로는 feature weight를 전부 고정하고, 최종 layer만을 사용하여 convexity가 보장된 logistic regression문제로 만듬.\n",
    "    - 보장된 건 아니지만, 비슷한 맥락으로 네트워크의 weight를 고정시키면 좀 더 안정적으로 IHVP를 얻을 가능성이 있음.\n",
    "2. Computational Complexity\n",
    "    - Weight가 많을 수록 계산이 복잡해지고, precision loss가 발생할 가능성이 늘어남."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.device import try_set_default_device, gpu\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hessian Vector Product\n",
    "\n",
    "def grad_inner_product(grad1, grad2):\n",
    "    # inner product for dictionary-format gradients (output scalar value)\n",
    "    \n",
    "    val = 0\n",
    "    \n",
    "    for ks in grad1.keys():\n",
    "        val += np.sum(np.multiply(grad1[ks],grad2[ks]))\n",
    "        \n",
    "    return val\n",
    "\n",
    "def weight_update(w, v, r):\n",
    "    # w: weights of neural network (tuple)\n",
    "    # v: value for delta w (dictionary, e.g., gradient value)\n",
    "    # r: hyperparameter for a gradient (scalar)\n",
    "\n",
    "    for p in w:\n",
    "        p.value += r * v[p]\n",
    "\n",
    "def HVP(y, x, v):\n",
    "    # Calculate Hessian vector product \n",
    "    # y: scalar function to be differentiated (function, e.g. cross entropy loss)\n",
    "    # x: feed_dict value for the network (dictionary, e.g. {model.X: image_batch, model.y: label_batch})\n",
    "    # v: vector to be producted (by Hessian) (numeric dictionary, e.g., g(z_test))\n",
    "    ## w: variables to differentiate (numeric, e.g. neural network weight)\n",
    "    \n",
    "    # hyperparameter r\n",
    "    r = 1e-2\n",
    "    \n",
    "    assert type(x)==dict, \"Input of HVP is wrong. this should be dictionary\"\n",
    "     \n",
    "    #w = y.parameters\n",
    "    w = v.keys()\n",
    "    \n",
    "    # gradient for plus\n",
    "    weight_update(w, v, +r)\n",
    "    g_plus = y.grad(x, wrt=w)\n",
    "  \n",
    "    # gradient for minus\n",
    "    weight_update(w, v, -2*r)\n",
    "    g_minus = y.grad(x, wrt=w)\n",
    "    \n",
    "    # weight reconstruction\n",
    "    weight_update(w, v, +r)\n",
    "    \n",
    "    hvp = {ks: (g_plus[ks] - g_minus[ks])/(2*r) for ks in g_plus.keys()}\n",
    "       \n",
    "    return hvp\n",
    "\n",
    "# Conjugate Gradient\n",
    "\n",
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def dic2vec(dic):\n",
    "    # convert a dictionary with matrix values to a 1D vector\n",
    "    # e.g. gradient of network -> 1D vector\n",
    "    vec = np.concatenate([val.reshape(-1) for val in dic.values()])\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def vec2dic(vec, fmt):\n",
    "    # convert a 1D vector to a dictionary of format fmt\n",
    "    # fmt = {key: val.shape for (key,val) in dict}\n",
    "    fmt_idx = [np.prod(val) for val in fmt.values()]\n",
    "    #lambda ls, idx: [ls[sum(idx[:i]):sum(idx[:i+1])] for i in range(len(idx))]\n",
    "    vec_split = [vec[sum(fmt_idx[:i]):sum(fmt_idx[:i+1])] for i in range(len(fmt_idx))]\n",
    "    dic = {key: vec_split[i].reshape(shape) for (i,(key,shape)) in enumerate(fmt.items())}\n",
    "\n",
    "    return dic\n",
    "\n",
    "def get_inverse_hvp_cg(model, y, v, data_set, **kwargs):\n",
    "    # return x, which is the solution of QP, whose value is H^-1 v\n",
    "    # kwargs: hyperparameters for conjugate gradient\n",
    "    batch_size = kwargs.pop('batch_size', 50)\n",
    "    damping = kwargs.pop('damping', 0.0)\n",
    "    maxiter = kwargs.pop('maxiter', 5e1)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "    def HVP_minibatch_val(y, v):\n",
    "        # Calculate Hessian vector product w.r.t whole dataset\n",
    "        # y: scalar function output of the neural network (e.g. model.loss)\n",
    "        # v: vector to be producted by inverse hessian (i.e.H^-1 v) (numeric dictionary, e.g. v_test)\n",
    "        \n",
    "        ## model: neural network model (e.g. model)\n",
    "        ## dataloader: training set dataloader\n",
    "        ## damping: damp term to make hessian convex\n",
    "\n",
    "        hvp_batch = {ks: [] for ks in v.keys()}\n",
    "\n",
    "        for img, lb in dataloader:\n",
    "            img = img.numpy(); lb = lb.numpy()\n",
    "            x_feed = {model.X: img, model.y:lb}\n",
    "            hvp = HVP(y,x_feed,v)\n",
    "            # add hvp value\n",
    "            [hvp_batch[ks].append(hvp[ks]/img.shape[0]) for ks in hvp.keys()]\n",
    "\n",
    "        hvp_mean = {ks: np.mean(hvp_batch[ks], axis=0) + damping*v[ks] for ks in hvp_batch.keys()}\n",
    "\n",
    "        return hvp_mean\n",
    "\n",
    "    def get_fmin_loss(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "\n",
    "        return 0.5 * grad_inner_product(hvp_val, x_dic) - grad_inner_product(v, x_dic)\n",
    "\n",
    "    def get_fmin_grad(x):\n",
    "        # x: 1D vector\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "        v_flat = dic2vec(v)\n",
    "\n",
    "        return hvp_flat - v_flat\n",
    "\n",
    "    fmin_loss_fn = get_fmin_loss\n",
    "    fmin_grad_fn = get_fmin_grad\n",
    "    \n",
    "    fmin_results = fmin_cg(f=get_fmin_loss, x0=dic2vec(v), fprime=fmin_grad_fn, maxiter=maxiter)\n",
    "    \n",
    "    return vec2dic(fmin_results, {key: val.shape for (key, val) in v.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IF_val(net, ihvp, data_set):\n",
    "    # Calculate influence function w.r.t ihvp and data_set\n",
    "    # This should be done in sample-wise, since the gradient operation will sum up over whole feed-dicted data\n",
    "    \n",
    "    # ihvp: inverse hessian vector product (dictionary)\n",
    "    # data_set: data_set to be feed to the gradient operation (dataset)\n",
    "    IF_list = []\n",
    "    \n",
    "    #params = net.logits.parameters\n",
    "    params = ihvp.keys()\n",
    "    \n",
    "    dataloader = DataLoader(data_set, 1, shuffle=False, num_workers=6)\n",
    "    \n",
    "    for img, lb in dataloader:\n",
    "        img = img.numpy(); lb = lb.numpy()\n",
    "        gd = net.loss.grad({net.X:img, net.y:lb}, wrt=params)\n",
    "        IF = grad_inner_product(ihvp, gd) / len(dataloader)\n",
    "        IF_list.append(IF)\n",
    "        \n",
    "    return IF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Parameter('W', [], [1 x 1]), Parameter('W', [], [1 x 1]), Parameter('W', [], [1 x 1]))\n",
      "Parameter('W', [], [1 x 1]) [[-0.52869767]]\n",
      "Parameter('W', [], [1 x 1]) [[-0.55389237]]\n",
      "Parameter('W', [], [1 x 1]) [[ 0.2377166]]\n",
      "loss = \n",
      " [ 0.7409308]\n",
      "(Parameter('W', [], [1 x 1]), Parameter('W', [], [1 x 1]))\n",
      "hvp_p(w1, w2, w3)\n",
      " {Parameter('W', [], [1 x 1]): array([[ 1.05137527]], dtype=float32), Parameter('W', [], [1 x 1]): array([[ 0.96631497]], dtype=float32), Parameter('W', [], [1 x 1]): array([[ 3.81159782]], dtype=float32)}\n",
      "hvp_p3(w2, w3)\n",
      " {Parameter('W', [], [1 x 1]): array([[ 0.47543347]], dtype=float32), Parameter('W', [], [1 x 1]): array([[ 0.47543347]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# toy example for inverse HVP (CG and SE)\n",
    "\n",
    "class SimpleNet(object):\n",
    "    def __init__(self):\n",
    "        self.X = C.input_variable(shape=(1,))\n",
    "        self.h = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.X)\n",
    "        self.h2 = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.h)\n",
    "        self.pred = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.h2)\n",
    "        self.y = C.input_variable(shape=(1,))\n",
    "        self.loss = C.squared_error(self.pred, self.y)\n",
    "        \n",
    "class SimpleDataset(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self._images, self._labels = images, labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self._images[index]\n",
    "        y = self._labels[index]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "params = net.pred.parameters\n",
    "\n",
    "x_feed = {net.X:np.array([[2.]],dtype=np.float32), net.y:np.array([[1.]],dtype=np.float32)}\n",
    "\n",
    "print(params)\n",
    "[print(pr, pr.value) for pr in params]\n",
    "print('loss = \\n', net.loss.eval(x_feed))\n",
    "# params[0].value = np.asarray([[1.]])\n",
    "# params[1].value = np.asarray([[1./3.]])\n",
    "# print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "\n",
    "images = np.asarray([[2.]], dtype=np.float32)\n",
    "labels = np.asarray([[1.]], dtype=np.float32)\n",
    "\n",
    "train_set = SimpleDataset(images,labels)\n",
    "\n",
    "p1 = net.h.parameters # w1\n",
    "p2 = net.h2.parameters # w1, w2\n",
    "print(p2)\n",
    "p3 = tuple(set(params) - set(p1)) # w2, w3\n",
    "\n",
    "v_p = {p: np.ones_like(p.value) for p in params}\n",
    "print('hvp_p(w1, w2, w3)\\n', HVP(net.loss, x_feed, v_p))\n",
    "# v_p1 = {p: np.ones_like(p.value) for p in p1}\n",
    "# print('hvp_p1(w1)\\n', HVP(net.loss, x_feed, v_p1))\n",
    "# v_p2 = {p: np.ones_like(p.value) for p in p2}\n",
    "# print('hvp_p2(w1, w2)\\n', HVP(net.loss, x_feed, v_p2))\n",
    "v_p3 = {p: np.ones_like(p.value) for p in p3}\n",
    "print('hvp_p3(w2, w3)\\n', HVP(net.pred, x_feed, v_p3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Parameter('W', [], [1 x 2]),) (Parameter('W', [], [2 x 1]),)\n",
      "{Parameter('W', [], [2 x 1]): array([[ 3.90574503],\n",
      "       [-5.65007734]], dtype=float32), Parameter('W', [], [1 x 2]): array([[ 4.20328236,  4.17423201]], dtype=float32)} [[ 4.20328236  4.17423201]] [[ 3.90574503]\n",
      " [-5.65007734]]\n"
     ]
    }
   ],
   "source": [
    "p1 = net.h.parameters\n",
    "p2 = tuple(set(params)-set(p1))\n",
    "\n",
    "print(p1, p2)\n",
    "\n",
    "v_p = net.loss.grad(x_feed, wrt=params)\n",
    "v_p1 = net.loss.grad(x_feed, wrt=p1)\n",
    "v_p2 = net.loss.grad(x_feed, wrt=p2)\n",
    "print(v_p, v_p1, v_p2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cntk]",
   "language": "python",
   "name": "conda-env-cntk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
