{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST DATASET\n",
    "\n",
    "load network\n",
    "check IF\n",
    "compare btw TF code\n",
    "check retrain & IF\n",
    "sorting & relabeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IHVP\n",
    "\n",
    "Algorithms for Inverse of Hessian Vector Product (CG & SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.device import try_set_default_device, gpu\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hessian Vector Product\n",
    "\n",
    "def grad_inner_product(grad1, grad2):\n",
    "    # inner product for dictionary-format gradients (output scalar value)\n",
    "    \n",
    "    val = 0\n",
    "    \n",
    "    for ks in grad1.keys():\n",
    "        val += np.sum(np.multiply(grad1[ks],grad2[ks]))\n",
    "        \n",
    "    return val\n",
    "\n",
    "def weight_update(w, v, r):\n",
    "    # w: weights of neural network (tuple)\n",
    "    # v: value for delta w (dictionary, e.g., gradient value)\n",
    "    # r: hyperparameter for a gradient (scalar)\n",
    "\n",
    "    for p in w:\n",
    "        p.value += r * v[p]\n",
    "\n",
    "def HVP(y, x, v):\n",
    "    # Calculate Hessian vector product \n",
    "    # y: scalar function to be differentiated (function, e.g. cross entropy loss)\n",
    "    # x: feed_dict value for the network (dictionary, e.g. {model.X: image_batch, model.y: label_batch})\n",
    "    # v: vector to be producted (by Hessian) (numeric dictionary, e.g., g(z_test))\n",
    "    ## w: variables to differentiate (numeric, e.g. neural network weight)\n",
    "    \n",
    "    # hyperparameter r\n",
    "    r = 1e-2\n",
    "    \n",
    "    assert type(x)==dict, \"Input of HVP is wrong. this should be dictionary\"\n",
    "     \n",
    "    w = y.parameters\n",
    "    \n",
    "    # gradient for plus\n",
    "    weight_update(w, v, +r)\n",
    "    g_plus = y.grad(x, wrt=params)\n",
    "  \n",
    "    # gradient for minus\n",
    "    weight_update(w, v, -2*r)\n",
    "    g_minus = y.grad(x, wrt=params)\n",
    "    \n",
    "    # weight reconstruction\n",
    "    weight_update(w, v, +r)\n",
    "    \n",
    "    hvp = {ks: (g_plus[ks] - g_minus[ks])/(2*r) for ks in g_plus.keys()}\n",
    "       \n",
    "    return hvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HVP_minibatch_val(model, y, v, dataloader):\n",
    "    # Calculate Hessian vector product w.r.t whole dataset\n",
    "    # model: neural network model (e.g. model)\n",
    "    # y: scalar function output of the neural network (e.g. model.loss)\n",
    "    # v: vector to be producted by inverse hessian (i.e.H^-1 v) (numeric dictionary, e.g. v_test)\n",
    "    # dataloader: training set dataloader\n",
    "    \n",
    "    # hyperparameters\n",
    "    damping = 0.0 # convexity term; paper ref:0.01\n",
    "    \n",
    "    hvp_batch = {ks: [] for ks in v.keys()}\n",
    "    \n",
    "    for img, lb in dataloader:\n",
    "        img = img.numpy(); lb = lb.numpy()\n",
    "        x_feed = {model.X: img, model.y:lb}\n",
    "        hvp = HVP(y,x_feed,v)\n",
    "        # add hvp value\n",
    "        [hvp_batch[ks].append(hvp[ks]/img.shape[0]) for ks in hvp.keys()]\n",
    "        \n",
    "    hvp_mean = {ks: np.mean(hvp_batch[ks], axis=0) + damping*v[ks] for ks in hvp_batch.keys()}\n",
    "    \n",
    "    return hvp_mean\n",
    "\n",
    "# x: solution vector for conjugate gradient, whose shape is same as flattened gradient. NOT feed dict value\n",
    "\n",
    "def dic2vec(dic):\n",
    "    # convert a dictionary with matrix values to a 1D vector\n",
    "    # e.g. gradient of network -> 1D vector\n",
    "    vec = np.concatenate([val.reshape(-1) for val in dic.values()])\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def vec2dic(vec, fmt):\n",
    "    # convert a 1D vector to a dictionary of format fmt\n",
    "    # fmt = {key: val.shape for (key,val) in dict}\n",
    "    fmt_idx = [np.prod(val) for val in fmt.values()]\n",
    "    #lambda ls, idx: [ls[sum(idx[:i]):sum(idx[:i+1])] for i in range(len(idx))]\n",
    "    vec_split = [vec[sum(fmt_idx[:i]):sum(fmt_idx[:i+1])] for i in range(len(fmt_idx))]\n",
    "    dic = {key: vec_split[i].reshape(shape) for (i,(key,shape)) in enumerate(fmt.items())}\n",
    "\n",
    "    return dic\n",
    "\n",
    "\n",
    "def get_fmin_loss_fn(model, y, v, dataloader):\n",
    "    \n",
    "    def get_fmin_loss(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(model, y, x_dic, dataloader)\n",
    "        \n",
    "        return 0.5 * grad_inner_product(hvp_val, x_dic) - grad_inner_product(v, x_dic)\n",
    "    \n",
    "    return get_fmin_loss\n",
    "\n",
    "def get_fmin_grad_fn(model, y, v, dataloader):\n",
    "    \n",
    "    def get_fmin_grad(x):\n",
    "        # x: 1D vector\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(model, y, x_dic, dataloader)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "        v_flat = dic2vec(v)\n",
    "        \n",
    "        return hvp_flat - v_flat\n",
    "    \n",
    "    return get_fmin_grad\n",
    "\n",
    "def get_fmin_hvp_fn(model, y, v, dataloader):\n",
    "\n",
    "    def get_fmin_hvp(x, p):\n",
    "        p_dic = vec2dic(p, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(model, y, p_dic, dataloader)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "\n",
    "        return hvp_flat\n",
    "    \n",
    "    return get_fmin_hvp\n",
    "\n",
    "def get_inverse_hvp_cg(model, y, v, data_set):\n",
    "    # return x, which is the solution of QP, whose value is H^-1 v\n",
    "\n",
    "    # hyperparameters\n",
    "    batch_size = 50\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    fmin_loss_fn = get_fmin_loss_fn(model, y, v, dataloader)\n",
    "    fmin_grad_fn = get_fmin_grad_fn(model, y, v, dataloader)\n",
    "    fmin_hvp_fn = get_fmin_hvp_fn(model, y, v, dataloader)\n",
    "    \n",
    "    fmin_results = fmin_ncg(\\\n",
    "            f = fmin_loss_fn, x0 = dic2vec(v), fprime = fmin_grad_fn,\\\n",
    "            fhess_p = fmin_hvp_fn, avextol = 1e-8, maxiter = 1e2)\n",
    "    \n",
    "    #return fmin_results\n",
    "    return vec2dic(fmin_results, {key: val.shape for (key, val) in v.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stochastic estimation\n",
    "def get_inverse_hvp_se(model, y, v, data_set, **kwargs):\n",
    "    # Calculate inverse hessian vector product over the training set\n",
    "    # model: neural network model (e.g. model)\n",
    "    # y: scalar function output of the neural network (e.g. model.loss)\n",
    "    # v: vector to be producted by inverse hessian (i.e.H^-1 v) (e.g. v_test)\n",
    "    # data_set: training set to be summed in Hessian\n",
    "    # kwargs: hyperparameters for stochastic estimation\n",
    "    recursion_depth = kwargs.pop('recursion_depth', 5) # epoch\n",
    "    scale = kwargs.pop('scale', 1e1) # similar to learning rate\n",
    "    damping = kwargs.pop('damping', 0.0) # paper reference: 0.01\n",
    "    batch_size = kwargs.pop('batch_size', 1)\n",
    "    num_samples = kwargs.pop('num_samples', 1) # the number of samples(:stochatic estimation of IF) to be averaged\n",
    "    verbose = kwargs.pop('verbose', False)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    inv_hvps = []\n",
    "    \n",
    "    params = y.parameters\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # obtain num_samples inverse hvps\n",
    "        cur_estimate = v\n",
    "        \n",
    "        for depth in range(recursion_depth):\n",
    "            # epoch-scale recursion depth\n",
    "            t1 = time.time()\n",
    "            for img, lb in dataloader:\n",
    "                img = img.numpy(); lb = lb.numpy()\n",
    "                x_feed = {model.X: img, model.y:lb}\n",
    "                hvp = HVP(y,x_feed,cur_estimate)\n",
    "                # cur_estimate = v + (1-damping)*cur_estimate + 1/scale*(hvp/batch_size)\n",
    "                cur_estimate = {ks: v[ks] + (1-damping/scale)*cur_estimate[ks] - (1/scale)*hvp[ks]/batch_size for ks in cur_estimate.keys()}\n",
    "            if verbose:\n",
    "                print('#w: \\n', list(map(lambda x: x.value, params)), '\\n#hvp: \\n', hvp, '\\n#ihvp: \\n', cur_estimate)\n",
    "                print(\"Recursion depth: {}, norm: {}, time: {} \\n\".format(depth, np.sqrt(grad_inner_product(cur_estimate,cur_estimate)),time.time()-t1))\n",
    "        \n",
    "        inv_hvp = {ks: (1/scale)*cur_estimate[ks] for ks in cur_estimate.keys()}\n",
    "        inv_hvps.append(inv_hvp)\n",
    "    \n",
    "    inv_hvp_val = {ks: np.mean([inv_hvps[i][ks] for i in range(num_samples)], axis=0) for ks in inv_hvps[0].keys()}\n",
    "    \n",
    "    return inv_hvp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = \n",
      " [[ 0.0370382]] \n",
      "w2 = \n",
      " [[-0.03171763]] \n",
      "loss = \n",
      " [ 1.00470448]\n",
      "w1 = \n",
      " [[ 1.]] \n",
      "w2 = \n",
      " [[ 0.33333334]] \n",
      "loss = \n",
      " [ 0.1111111]\n",
      "hvp {Parameter('W', [], [1 x 1]): array([[ 2.22302079]], dtype=float32), Parameter('W', [], [1 x 1]): array([[ 9.33413506]], dtype=float32)}\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -0.583325\n",
      "         Iterations: 5\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 19\n",
      "         Hessian evaluations: 12\n",
      "inverse hvp [ 1.24996293 -0.08331362]\n",
      "inverse hvp {Parameter('W', [], [1 x 1]): array([[ 1.05965364]], dtype=float32), Parameter('W', [], [1 x 1]): array([[-0.04848924]], dtype=float32)}\n",
      "inverse hvp {Parameter('W', [], [1 x 1]): array([[ 1.08393216]], dtype=float32), Parameter('W', [], [1 x 1]): array([[-0.05433288]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# toy example for inverse HVP (CG and SE)\n",
    "\n",
    "class SimpleNet(object):\n",
    "    def __init__(self):\n",
    "        self.X = C.input_variable(shape=(1,))\n",
    "        self.h = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.X)\n",
    "        self.pred = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.h)\n",
    "        self.y = C.input_variable(shape=(1,))\n",
    "        #self.loss = C.reduce_l2(self.pred-self.y)\n",
    "        self.loss = C.squared_error(self.pred, self.y)\n",
    "        \n",
    "class SimpleDataset(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self._images, self._labels = images, labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self._images[index]\n",
    "        y = self._labels[index]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "params = net.pred.parameters\n",
    "\n",
    "x_feed = {net.X:np.array([[2.]],dtype=np.float32), net.y:np.array([[1.]],dtype=np.float32)}\n",
    "v_feed = {p: np.ones_like(p.value) for p in params}\n",
    "\n",
    "print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "params[0].value = np.asarray([[1.]])\n",
    "params[1].value = np.asarray([[1./3.]])\n",
    "print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "\n",
    "print('hvp', HVP(net.loss, x_feed, v_feed))\n",
    "\n",
    "images = np.asarray([[2.],[2.]], dtype=np.float32)\n",
    "labels = np.asarray([[1.],[1.]], dtype=np.float32)\n",
    "#images = np.asarray([[2.]], dtype=np.float32)\n",
    "#labels = np.asarray([[1.]], dtype=np.float32)\n",
    "\n",
    "train_set = SimpleDataset(images,labels)\n",
    "\n",
    "print('inverse hvp', get_inverse_hvp_cg(net, net.loss, v_feed, train_set))\n",
    "print('inverse hvp', get_inverse_hvp_se(net, net.loss, v_feed, train_set,**{'scale':20, 'damping':0.1}))\n",
    "print('inverse hvp', get_inverse_hvp_se(net, net.loss, v_feed, train_set,**{'scale':10, 'damping':0.1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../refer/boot_strapping')\n",
    "import json\n",
    "\n",
    "from datasets import dataset28 as dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image_from_data(img):\n",
    "    # show image from dataset\n",
    "    # img: (C,W,H) numpy array\n",
    "    img_show = np.squeeze(np.transpose(img, [1,2,0]))\n",
    "    imshow(img_show)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 <class 'datasets.dataset28.LazyDataset'>\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# emnist dataset\n",
    "root_dir = '/Data/emnist/balanced/original'\n",
    "\n",
    "# sample sized\n",
    "trainval_list, anno_dict = dataset.read_data_subset(root_dir, mode='train1', sample_size=1000)\n",
    "test_list, _ = dataset.read_data_subset(root_dir, mode='validation1', sample_size=1000)\n",
    "\n",
    "with open('/Data/emnist/balanced/original/annotation/annotation1_wp_0.3.json','r') as fid:\n",
    "    noisy_anno_dict = json.load(fid)\n",
    "\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, noisy_anno_dict)\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "# emnist dataset: SANITY CHECK\n",
    "print(len(test_set), type(test_set))\n",
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.shape (64, 28, 28)\n",
      "pool1.shape (64, 14, 14)\n",
      "conv2.shape (128, 14, 14)\n",
      "pool2.shape (128, 7, 7)\n",
      "conv3.shape (256, 7, 7)\n",
      "pool3.shape (256, 4, 4)\n",
      "Test error rate: 0.11570581896551724\n",
      "Total tack time(sec): 9.851991653442383\n",
      "Tact time per image(sec): 0.009851991653442383\n",
      "Confusion matrix: \n",
      "[[18  0  0 ...,  0  0  0]\n",
      " [ 0 18  0 ...,  0  0  0]\n",
      " [ 0  0 15 ...,  0  0  0]\n",
      " ..., \n",
      " [ 0  0  0 ..., 17  0  0]\n",
      " [ 0  0  0 ...,  0 18  0]\n",
      " [ 0  1  0 ...,  0  0 18]]\n"
     ]
    }
   ],
   "source": [
    "# emnist network\n",
    "from models.nn import VGG as ConvNet\n",
    "\n",
    "hp_d = dict() # hyperparameters for a network\n",
    "net = ConvNet(train_set.__getitem__(0)[0].shape, len(anno_dict['classes']), **hp_d)\n",
    "net.logits.restore('/Data/checkpts/noisy/model_fold_1_trainval_ratio_1.0.dnn')\n",
    "\n",
    "# emnist network: SANITY CHECK\n",
    "start_time = time.time()\n",
    "ys, y_preds, test_score, confusion_matrix = net.predict(test_set, **hp_d)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print('Test error rate: {}'.format(test_score))\n",
    "print('Total tack time(sec): {}'.format(total_time))\n",
    "print('Tact time per image(sec): {}'.format(total_time / len(test_list)))\n",
    "print('Confusion matrix: \\n{}'.format(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA0kRfiApwg8kRfiBpP4Pm4UKVx1XQfsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth label:  L\n",
      "network prediction:  I\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.3030843472\n",
      "3505.32309139\n"
     ]
    }
   ],
   "source": [
    "# hessian vector product w.r.t. test image\n",
    "\n",
    "params = net.logits.parameters\n",
    "\n",
    "img_test, lb_test = test_set.__getitem__(1)\n",
    "show_image_from_data(img_test)\n",
    "\n",
    "print('ground truth label: ', anno_dict['classes'][str(np.argmax(lb_test))])\n",
    "print('network prediction: ', anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))])\n",
    "\n",
    "v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "\n",
    "# HVP(y,x,v)\n",
    "img, lb = train_set.__getitem__(1)\n",
    "img.shape\n",
    "show_image_from_data(img)\n",
    "\n",
    "v_train = net.loss.grad({net.X:img, net.y:lb}, wrt=params)\n",
    "\n",
    "hvp = HVP(net.loss, {net.X:img, net.y:lb}, v_test)\n",
    "\n",
    "print(grad_inner_product(hvp, v_test))\n",
    "print(grad_inner_product(v_train, v_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Parameter('W', [], [2048 x 47]), Parameter('b', [], [47]), Parameter('W', [], [256 x 4 x 4 x 2048]), Parameter('b', [], [2048]), Parameter('W', [], [256 x 128 x 3 x 3]), Parameter('b', [], [256 x 1 x 1]), Parameter('W', [], [128 x 64 x 3 x 3]), Parameter('b', [], [128 x 1 x 1]), Parameter('W', [], [64 x 1 x 3 x 3]), Parameter('b', [], [64 x 1 x 1]))\n",
      "(Parameter('W', [], [2048 x 47]), Parameter('b', [], [47]), Parameter('W', [], [256 x 4 x 4 x 2048]), Parameter('b', [], [2048]), Parameter('W', [], [256 x 128 x 3 x 3]), Parameter('b', [], [256 x 1 x 1]), Parameter('W', [], [128 x 64 x 3 x 3]), Parameter('b', [], [128 x 1 x 1]), Parameter('W', [], [64 x 1 x 3 x 3]), Parameter('b', [], [64 x 1 x 1]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hwehee/anaconda3/envs/cntk/lib/python3.5/site-packages/ipykernel/__main__.py:46: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    }
   ],
   "source": [
    "img, lb = train_set.__getitem__(1)\n",
    "show_image_from_data(img)\n",
    "\n",
    "v_train = net.loss.grad({net.X:img, net.y:lb}, wrt=params)\n",
    "\n",
    "net.logits.restore('/Data/checkpts/noisy/model_fold_1_trainval_ratio_1.0.dnn')\n",
    "net.loss.eval({net.X: img, net.y:lb})\n",
    "\n",
    "print(net.loss.parameters)\n",
    "print(net.logits.parameters)\n",
    "\n",
    "t1 = time.time()\n",
    "ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set)\n",
    "IF_cg = grad_inner_product(ihvp_cg, v_train)/1000 # loss difference = -1/num_sample * influence function\n",
    "print('it takes {} sec, and its value {}'.format(time.time()-t1, IF_cg))\n",
    "\n",
    "#t1 = time.time()\n",
    "#ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':20, 'damping':0.1, 'verbose':True})\n",
    "#IF_se = grad_inner_product(ihvp_se, v_train)/1000 # loss difference = -1/num_sample * influence function\n",
    "#print('it takes {} sec'.format(time.time()-t1))\n",
    "\n",
    "#print(IF_cg, IF_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sorting with IF\n",
    "\n",
    "# 1: v_test^T * v_train\n",
    "# 2: v_cg^T * v_train\n",
    "# 3: v_se^T * v_train\n",
    "\n",
    "# visualization w.r.t. top 5 sampling"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cntk]",
   "language": "python",
   "name": "conda-env-cntk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
