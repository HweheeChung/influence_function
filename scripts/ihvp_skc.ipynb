{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.device import try_set_default_device, gpu\n",
    "try_set_default_device(gpu(0))\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hessian Vector Product\n",
    "\n",
    "def grad_inner_product(grad1, grad2):\n",
    "    # inner product for dictionary-format gradients (output scalar value)\n",
    "    \n",
    "    val = 0\n",
    "    \n",
    "    assert(len(grad1)==len(grad2))\n",
    "    \n",
    "    for ks in grad1.keys():\n",
    "        val += np.sum(np.multiply(grad1[ks],grad2[ks]))\n",
    "        \n",
    "    return val\n",
    "\n",
    "def weight_update(w, v, r):\n",
    "    # w: weights of neural network (tuple)\n",
    "    # v: value for delta w (dictionary, e.g., gradient value)\n",
    "    # r: hyperparameter for a gradient (scalar)\n",
    "\n",
    "    for p in w:\n",
    "        p.value += r * v[p]\n",
    "\n",
    "def HVP(y, x, v):\n",
    "    # Calculate Hessian vector product \n",
    "    # y: scalar function to be differentiated (function, e.g. cross entropy loss)\n",
    "    # x: feed_dict value for the network (dictionary, e.g. {model.X: image_batch, model.y: label_batch})\n",
    "    # v: vector to be producted (by Hessian) (numeric dictionary, e.g., g(z_test))\n",
    "    ## w: variables to differentiate (numeric, e.g. neural network weight)\n",
    "    \n",
    "    # hyperparameter r\n",
    "    r = 1e-2\n",
    "    \n",
    "    assert type(x)==dict, \"Input of HVP is wrong. 'x' should be dictionary(feed dict format)\"\n",
    "    assert type(v)==dict, \"Input of HVP is wrong. 'v' should be dictionary(weight:value format)\"\n",
    "\n",
    "    w = v.keys()\n",
    "    \n",
    "    # gradient for plus\n",
    "    weight_update(w, v, +r)\n",
    "    g_plus = y.grad(x, wrt=w)\n",
    "  \n",
    "    # gradient for minus\n",
    "    weight_update(w, v, -2*r)\n",
    "    g_minus = y.grad(x, wrt=w)\n",
    "    \n",
    "    # weight reconstruction\n",
    "    weight_update(w, v, +r)\n",
    "    \n",
    "    hvp = {ks: (g_plus[ks] - g_minus[ks])/(2*r) for ks in g_plus.keys()}\n",
    "       \n",
    "    return hvp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Newton-Conjugate Gradient\n",
    "\n",
    "from scipy.optimize import fmin_ncg\n",
    "\n",
    "def dic2vec(dic):\n",
    "    # convert a dictionary with matrix values to a 1D vector\n",
    "    # e.g. gradient of network -> 1D vector\n",
    "    vec = np.concatenate([val.reshape(-1) for val in dic.values()])\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def vec2dic(vec, fmt):\n",
    "    # convert a 1D vector to a dictionary of format fmt\n",
    "    # fmt = {key: val.shape for (key,val) in dict}\n",
    "    fmt_idx = [np.prod(val) for val in fmt.values()]\n",
    "    #lambda ls, idx: [ls[sum(idx[:i]):sum(idx[:i+1])] for i in range(len(idx))]\n",
    "    vec_split = [vec[sum(fmt_idx[:i]):sum(fmt_idx[:i+1])] for i in range(len(fmt_idx))]\n",
    "    dic = {key: vec_split[i].reshape(shape) for (i,(key,shape)) in enumerate(fmt.items())}\n",
    "\n",
    "    return dic\n",
    "\n",
    "def get_inverse_hvp_ncg(model, y, v, data_set, **kwargs):\n",
    "    # return x, which is the solution of QP, whose value is H^-1 v\n",
    "    # kwargs: hyperparameters for conjugate gradient\n",
    "    batch_size = kwargs.pop('batch_size', 50)\n",
    "    damping = kwargs.pop('damping', 0.0)\n",
    "    avextol = kwargs.pop('avextol', 1e-8)\n",
    "    maxiter = kwargs.pop('maxiter', 1e1)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    get_inverse_hvp_ncg.cnt = 0\n",
    "\n",
    "    def HVP_minibatch_val(y, v):\n",
    "        # Calculate Hessian vector product w.r.t whole dataset\n",
    "        # y: scalar function output of the neural network (e.g. model.loss)\n",
    "        # v: vector to be producted by inverse hessian (i.e.H^-1 v) (numeric dictionary, e.g. v_test)\n",
    "        \n",
    "        ## model: neural network model (e.g. model)\n",
    "        ## dataloader: training set dataloader\n",
    "        ## damping: damp term to make hessian convex\n",
    "\n",
    "        hvp_batch = {ks: [] for ks in v.keys()}\n",
    "\n",
    "        for img, lb in dataloader:\n",
    "            img = img.numpy(); lb = lb.numpy()\n",
    "            x_feed = {model.X: img, model.y:lb}\n",
    "            hvp = HVP(y,x_feed,v)\n",
    "            # add hvp value\n",
    "            [hvp_batch[ks].append(hvp[ks]/img.shape[0]) for ks in hvp.keys()]\n",
    "\n",
    "        hvp_mean = {ks: np.mean(hvp_batch[ks], axis=0) + damping*v[ks] for ks in hvp_batch.keys()}\n",
    "\n",
    "        return hvp_mean\n",
    "\n",
    "    def get_fmin_loss(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "\n",
    "        return 0.5 * grad_inner_product(hvp_val, x_dic) - grad_inner_product(v, x_dic)\n",
    "\n",
    "    def get_fmin_grad(x):\n",
    "        # x: 1D vector\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "        v_flat = dic2vec(v)\n",
    "\n",
    "        return hvp_flat - v_flat\n",
    "    \n",
    "    def get_fmin_hvp(x, p):\n",
    "        p_dic = vec2dic(p, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, p_dic)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "\n",
    "        return hvp_flat\n",
    "\n",
    "    def ncg_callback(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        print('iteration: {}'.format(get_inverse_hvp_ncg.cnt), ', ', time.time()-t0, '(sec) elapsed')\n",
    "        print('vector element-wise square: ', grad_inner_product(x_dic, x_dic))\n",
    "        get_inverse_hvp_ncg.cnt += 1\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    fmin_loss_fn = get_fmin_loss\n",
    "    fmin_grad_fn = get_fmin_grad\n",
    "    fmin_hvp_fn = get_fmin_hvp\n",
    "    \n",
    "    fmin_results = fmin_ncg(\\\n",
    "            f = fmin_loss_fn, x0 = dic2vec(v), fprime = fmin_grad_fn,\\\n",
    "            fhess_p = fmin_hvp_fn, avextol = avextol, maxiter = maxiter, callback=ncg_callback)\n",
    "    \n",
    "    return vec2dic(fmin_results, {key: val.shape for (key, val) in v.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Conjugate Gradient\n",
    "\n",
    "from scipy.optimize import fmin_cg\n",
    "\n",
    "def dic2vec(dic):\n",
    "    # convert a dictionary with matrix values to a 1D vector\n",
    "    # e.g. gradient of network -> 1D vector\n",
    "    vec = np.concatenate([val.reshape(-1) for val in dic.values()])\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def vec2dic(vec, fmt):\n",
    "    # convert a 1D vector to a dictionary of format fmt\n",
    "    # fmt = {key: val.shape for (key,val) in dict}\n",
    "    fmt_idx = [np.prod(val) for val in fmt.values()]\n",
    "    #lambda ls, idx: [ls[sum(idx[:i]):sum(idx[:i+1])] for i in range(len(idx))]\n",
    "    vec_split = [vec[sum(fmt_idx[:i]):sum(fmt_idx[:i+1])] for i in range(len(fmt_idx))]\n",
    "    dic = {key: vec_split[i].reshape(shape) for (i,(key,shape)) in enumerate(fmt.items())}\n",
    "\n",
    "    return dic\n",
    "\n",
    "def get_inverse_hvp_cg(model, y, v, data_set, **kwargs):\n",
    "    # return x, which is the solution of QP, whose value is H^-1 v\n",
    "    # kwargs: hyperparameters for conjugate gradient\n",
    "    batch_size = kwargs.pop('batch_size', 50)\n",
    "    damping = kwargs.pop('damping', 0.0)\n",
    "    maxiter = kwargs.pop('maxiter', 5e1)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    get_inverse_hvp_cg.cnt = 0\n",
    "\n",
    "    def HVP_minibatch_val(y, v):\n",
    "        # Calculate Hessian vector product w.r.t whole dataset\n",
    "        # y: scalar function output of the neural network (e.g. model.loss)\n",
    "        # v: vector to be producted by inverse hessian (i.e.H^-1 v) (numeric dictionary, e.g. v_test)\n",
    "        \n",
    "        ## model: neural network model (e.g. model)\n",
    "        ## dataloader: training set dataloader\n",
    "        ## damping: damp term to make hessian convex\n",
    "        \n",
    "        num_data = data_set.__len__()\n",
    "\n",
    "        hvp_batch = {key: np.zeros_like(value) for key,value in v.items()}\n",
    "\n",
    "        for img, lb in dataloader:\n",
    "            img = img.numpy(); lb = lb.numpy()\n",
    "            x_feed = {model.X: img, model.y:lb}\n",
    "            hvp = HVP(y,x_feed,v)\n",
    "            # add hvp value\n",
    "            for ks in hvp.keys():\n",
    "                hvp_batch[ks] += hvp[ks]/num_data\n",
    "                \n",
    "        return hvp_batch\n",
    "\n",
    "    def fmin_loss_fn(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "\n",
    "        return 0.5 * grad_inner_product(hvp_val, x_dic) - grad_inner_product(v, x_dic)\n",
    "\n",
    "    def fmin_grad_fn(x):\n",
    "        # x: 1D vector\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        hvp_val = HVP_minibatch_val(y, x_dic)\n",
    "        hvp_flat = dic2vec(hvp_val)\n",
    "        v_flat = dic2vec(v)\n",
    "\n",
    "        return hvp_flat - v_flat\n",
    "    \n",
    "    def cg_callback(x):\n",
    "        x_dic = vec2dic(x, {key: val.shape for (key, val) in v.items()})\n",
    "        print('iteration: {}'.format(get_inverse_hvp_cg.cnt), ', ', time.time()-t0, '(sec) elapsed')\n",
    "        print('vector element-wise square: ', grad_inner_product(x_dic, x_dic))\n",
    "        get_inverse_hvp_cg.cnt += 1\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    fmin_results = fmin_cg(f=fmin_loss_fn, x0=dic2vec(v), fprime=fmin_grad_fn, callback=cg_callback, maxiter=maxiter)\n",
    "    \n",
    "    return vec2dic(fmin_results, {key: val.shape for (key, val) in v.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stochastic Estimation\n",
    "\n",
    "def get_inverse_hvp_se(model, y, v, data_set, **kwargs):\n",
    "    # Calculate inverse hessian vector product over the training set\n",
    "    # model: neural network model (e.g. model)\n",
    "    # y: scalar function output of the neural network (e.g. model.loss)\n",
    "    # v: vector to be producted by inverse hessian (i.e.H^-1 v) (e.g. v_test)\n",
    "    # data_set: training set to be summed in Hessian\n",
    "    # kwargs: hyperparameters for stochastic estimation\n",
    "    \n",
    "    # hyperparameters\n",
    "    recursion_depth = kwargs.pop('recursion_depth', 50) # epoch\n",
    "    scale = kwargs.pop('scale', 1e1) # similar to learning rate\n",
    "    damping = kwargs.pop('damping', 0.0) # paper reference: 0.01\n",
    "    batch_size = kwargs.pop('batch_size', 1)\n",
    "    num_samples = kwargs.pop('num_samples', 1) # the number of samples(:stochatic estimation of IF) to be averaged\n",
    "    tolerance = kwargs.pop('tolerance', 1e-2) # the difference btw l2 norms of current and previous vector used for early stopping\n",
    "    verbose = kwargs.pop('verbose', False)\n",
    "    \n",
    "    dataloader = DataLoader(data_set, batch_size, shuffle=True, num_workers=6)\n",
    "    \n",
    "    inv_hvps = []\n",
    "    \n",
    "    params = v.keys()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # obtain num_samples inverse hvps\n",
    "        cur_estimate = v\n",
    "        prev_norm = 0\n",
    "        \n",
    "        for depth in range(recursion_depth):\n",
    "            # epoch-scale recursion depth\n",
    "            t1 = time.time()\n",
    "            for img, lb in dataloader:\n",
    "                img = img.numpy(); lb = lb.numpy()\n",
    "                x_feed = {model.X: img, model.y:lb}\n",
    "                hvp = HVP(y,x_feed,cur_estimate)\n",
    "                # cur_estimate = v + (1-damping)*cur_estimate + 1/scale*(hvp/batch_size)\n",
    "                cur_estimate = {ks: v[ks] + (1-damping/scale)*cur_estimate[ks] - (1/scale)*hvp[ks]/batch_size for ks in cur_estimate.keys()}\n",
    "            \n",
    "            if verbose:\n",
    "                print('#w: \\n', list(map(lambda x: x.value, params)), '\\n#hvp: \\n', hvp, '\\n#ihvp: \\n', cur_estimate)\n",
    "            \n",
    "            cur_norm = np.sqrt(grad_inner_product(cur_estimate,cur_estimate))\n",
    "            print('Recursion depth: {}, norm: {}, time: {} \\n'.format(depth, cur_norm,time.time()-t1))\n",
    "            \n",
    "            # divergence check\n",
    "            if np.isnan(cur_norm):\n",
    "                print('## The result has been diverged ##')\n",
    "                break\n",
    "            \n",
    "            # convergence check\n",
    "            if np.abs(cur_norm - prev_norm) <= tolerance:\n",
    "                # change this to more precise one (<- scipy.fmin_cg also use gnorm)\n",
    "                print('## Early stopped due to small change')\n",
    "                break\n",
    "            prev_norm = cur_norm\n",
    "        \n",
    "        inv_hvp = {ks: (1/scale)*cur_estimate[ks] for ks in cur_estimate.keys()}\n",
    "        inv_hvps.append(inv_hvp)\n",
    "    \n",
    "    inv_hvp_val = {ks: np.mean([inv_hvps[i][ks] for i in range(num_samples)], axis=0) for ks in inv_hvps[0].keys()}\n",
    "    \n",
    "    return inv_hvp_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# toy example for inverse HVP (CG, NCG and SE)\n",
    "\n",
    "class SimpleNet(object):\n",
    "    def __init__(self):\n",
    "        self.X = C.input_variable(shape=(1,))\n",
    "        self.h = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.X)\n",
    "        self.pred = C.layers.Dense(1, activation=None, init=C.uniform(1), bias=False)(self.h)\n",
    "        self.y = C.input_variable(shape=(1,))\n",
    "        self.loss = C.squared_error(self.pred, self.y)\n",
    "        \n",
    "class SimpleDataset(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self._images, self._labels = images, labels\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self._images[index]\n",
    "        y = self._labels[index]\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._images)\n",
    "\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "params = net.pred.parameters\n",
    "\n",
    "x_feed = {net.X:np.array([[2.]],dtype=np.float32), net.y:np.array([[1.]],dtype=np.float32)}\n",
    "v_feed = {p: np.ones_like(p.value) for p in params}\n",
    "\n",
    "print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "params[0].value = np.asarray([[1.]])\n",
    "params[1].value = np.asarray([[1./3.]])\n",
    "print('w1 = \\n', params[0].value, '\\nw2 = \\n', params[1].value, '\\nloss = \\n', net.loss.eval(x_feed))\n",
    "\n",
    "print('hvp', HVP(net.loss, x_feed, v_feed))\n",
    "\n",
    "#images = np.asarray([[2.],[2.]], dtype=np.float32)\n",
    "#labels = np.asarray([[1.],[1.]], dtype=np.float32)\n",
    "images = np.asarray([[2.]], dtype=np.float32)\n",
    "labels = np.asarray([[1.]], dtype=np.float32)\n",
    "\n",
    "train_set = SimpleDataset(images,labels)\n",
    "\n",
    "print('######## damping = 0.0, desired solution: [1.25, -0.08] ########'); t1 = time.time()\n",
    "ihvp_ncg = get_inverse_hvp_ncg(net, net.loss, v_feed, train_set, **{'damping': 0.0}); t2 = time.time()\n",
    "ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_feed, train_set, **{'damping': 0.0}); t3 = time.time()\n",
    "ihvp_se = get_inverse_hvp_se(net, net.loss, v_feed, train_set, **{'damping': 0.0, 'recursion_depth': 100}); t4 = time.time()\n",
    "print('inverse hvp_ncg', ihvp_ncg, '\\ntime: ', t2-t1)\n",
    "print('inverse hvp_cg', ihvp_cg, '\\ntime: ', t3-t2 )\n",
    "print('inverse hvp_se', ihvp_se, '\\ntime: ', t4-t3)\n",
    "\n",
    "# print('inverse hvp_ncg', get_inverse_hvp_ncg(net, net.loss, v_feed, train_set, **{'damping': 0.1}))\n",
    "# print('inverse hvp_cg', get_inverse_hvp_cg(net, net.loss, v_feed, train_set, **{'damping': 0.1}))\n",
    "# print('inverse hvp_se', get_inverse_hvp_se(net, net.loss, v_feed, train_set, **{'scale':10, 'damping':0.1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('/Data/github/data_analysis/dataset-analysis-new/')\n",
    "import json\n",
    "\n",
    "from datasets import dataset as dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import scipy.misc\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_image_from_data(img):\n",
    "    # show image from dataset\n",
    "    # img: (C,W,H) numpy array\n",
    "    img_show = np.squeeze(np.transpose(img, [1,2,0]))\n",
    "    imshow(img_show)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IF_val(net, ihvp, data_set, cosine=False):\n",
    "    # Calculate influence function w.r.t ihvp and data_set\n",
    "    # This should be done in sample-wise, since the gradient operation will sum up over whole feed-dicted data\n",
    "    \n",
    "    # ihvp: inverse hessian vector product (dictionary)\n",
    "    # data_set: data_set to be feed to the gradient operation (dataset)\n",
    "    IF_list = []\n",
    "    \n",
    "    #params = net.logits.parameters\n",
    "    params = ihvp.keys()\n",
    "    \n",
    "    dataloader = DataLoader(data_set, 1, shuffle=False, num_workers=6)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    for img, lb in dataloader:\n",
    "        img = img.numpy(); lb = lb.numpy()\n",
    "        gd = net.loss.grad({net.X:img, net.y:lb}, wrt=params)\n",
    "        if cosine:\n",
    "            nrm = np.sqrt(grad_inner_product(gd,gd))\n",
    "            gd = {k: v/nrm for k,v in gd.items()}\n",
    "        IF = -grad_inner_product(ihvp, gd) / len(dataloader)\n",
    "        IF_list.append(IF)\n",
    "    print('IF_val takes {} sec'.format(time.time()-t1))\n",
    "        \n",
    "    return IF_list\n",
    "\n",
    "def visualize_topk_samples(measure, train_set, num_sample=5, mask=None, verbose='ALL', save_path='./result'):\n",
    "    # 'ALL': show DISADV / ADV / INF / NEG examples\n",
    "    # 'ADV': show ADV only\n",
    "    # 'DIS': show DIS only\n",
    "\n",
    "    axis = 0 # axis=0 -> column output / axis=1 -> row output\n",
    "    \n",
    "    if mask == None:\n",
    "        argsort = np.argsort(measure)\n",
    "    else:\n",
    "        assert(len(mask) == len(measure))\n",
    "        argsort = list(filter(lambda idx: mask[idx], np.argsort(measure)))\n",
    "    \n",
    "    topk = argsort[-1:-num_sample-1:-1]\n",
    "    botk = argsort[0:num_sample]\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    if verbose == 'DIS' or verbose == 'ALL':\n",
    "        dis = []\n",
    "        true_label = ''\n",
    "        print('\\n## SHOW {}-MOST DISADVANTAGEOUS EXAMPLES ##\\n'.format(num_sample))\n",
    "        for idx in topk:\n",
    "            img, lb = train_set.__getitem__(idx)\n",
    "            show_image_from_data(img)\n",
    "            print('training set name: ', train_set.filename_list[idx])\n",
    "            print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "            print('IF measure: ', measure[idx])\n",
    "            print(trainval_list[idx])\n",
    "            dis.append(np.transpose(img,(1,2,0)))\n",
    "            true_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "        dis = np.squeeze(np.concatenate(dis, axis=axis))\n",
    "        scipy.misc.imsave(save_path+'/disadvantageous_true_{}.png'.format(true_label), dis)\n",
    "\n",
    "    if verbose == 'ADV' or verbose == 'ALL':\n",
    "        adv = []\n",
    "        true_label = ''\n",
    "        print('\\n## SHOW {}-MOST ADVANTAGEOUS EXAMPLES ##\\n'.format(num_sample))\n",
    "        for idx in botk:\n",
    "            img, lb = train_set.__getitem__(idx)\n",
    "            show_image_from_data(img)\n",
    "            print('training set name: ', train_set.filename_list[idx])\n",
    "            print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "            print('IF measure: ', measure[idx])\n",
    "            print(trainval_list[idx])\n",
    "            adv.append(np.transpose(img,(1,2,0)))\n",
    "            true_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "        adv = np.squeeze(np.concatenate(adv, axis=axis))\n",
    "        scipy.misc.imsave(save_path+'/advantageous_true_{}.png'.format(true_label), adv)\n",
    "\n",
    "    if verbose == 'ALL':\n",
    "        \n",
    "        if mask == None:\n",
    "            argsort_abs = np.argsort(np.abs(measure))\n",
    "        else:\n",
    "            assert(len(mask) == len(measure))\n",
    "            argsort_abs = list(filter(lambda idx: mask[idx], np.argsort(np.abs(measure))))\n",
    "\n",
    "        topk_abs = argsort_abs[-1:-num_sample-1:-1]\n",
    "        botk_abs = argsort_abs[0:num_sample]\n",
    "        \n",
    "        inf = []\n",
    "        true_label = ''\n",
    "        print('\\n## SHOW {}-MOST INFLUENTIAL EXAMPLES ##\\n'.format(num_sample))\n",
    "        for idx in topk_abs:\n",
    "            img, lb = train_set.__getitem__(idx)\n",
    "            show_image_from_data(img)\n",
    "            print('training set name: ', train_set.filename_list[idx])\n",
    "            print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "            print('IF measure: ', measure[idx])\n",
    "            inf.append(np.transpose(img,(1,2,0)))\n",
    "            true_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "        inf = np.squeeze(np.concatenate(inf, axis=axis))\n",
    "        scipy.misc.imsave(save_path+'/influential_true_{}.png'.format(true_label), inf)\n",
    "\n",
    "        neg = []\n",
    "        true_label = ''\n",
    "        print('\\n## SHOW {}-MOST NEGLIGIBLE EXAMPLES ##\\n'.format(num_sample))\n",
    "        for idx in botk_abs:\n",
    "            img, lb = train_set.__getitem__(idx)\n",
    "            show_image_from_data(img)\n",
    "            print('training set name: ', train_set.filename_list[idx])\n",
    "            print('training set label: ', train_set.anno_dict['classes'][str(np.argmax(lb))])\n",
    "            print('IF measure: ', measure[idx])\n",
    "            neg.append(np.transpose(img,(1,2,0)))\n",
    "            true_label += train_set.anno_dict['classes'][str(np.argmax(lb))]\n",
    "        neg = np.squeeze(np.concatenate(neg, axis=axis))\n",
    "        scipy.misc.imsave(save_path+'/negligible_true_{}.png'.format(true_label), neg)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 <class 'datasets.dataset.LazyDataset'>\n",
      "55647\n"
     ]
    }
   ],
   "source": [
    "# skc dataset\n",
    "root_dir = '/Data/skc/20180424/original'\n",
    "\n",
    "# sample size\n",
    "#trainval_list, anno_dict = dataset.read_data_subset(root_dir, mode='train1', sample_size=10000)\n",
    "trainval_list, anno_dict = dataset.read_data_subset(root_dir, mode='train1')\n",
    "test_list, _ = dataset.read_data_subset(root_dir, mode='validation1', sample_size=500)\n",
    "\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "# emnist dataset: SANITY CHECK\n",
    "print(len(test_set), type(test_set))\n",
    "print(len(trainval_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skc network\n",
    "from models.nn import ResNet_18 as ConvNet\n",
    "\n",
    "img, lb = test_set.__getitem__(0)\n",
    "C, H, W = img.shape\n",
    "\n",
    "hp_d = dict() # hyperparameters for a network\n",
    "mean = np.load('/Data/github/data_analysis/dataset-analysis-new/output/mean_skc.npy')\n",
    "hp_d['image_mean'] = np.transpose(np.tile(mean,(H,W,1)),(2,0,1))\n",
    "\n",
    "net = ConvNet((C,H,W), len(anno_dict['classes']), **hp_d)\n",
    "net.logits.restore('/Data/checkpts/skc/model_fold_1_trainval_ratio_1.0.dnn')\n",
    "\n",
    "# skc network: SANITY CHECK\n",
    "start_time = time.time()\n",
    "ys, y_preds, test_score, confusion_matrix = net.predict(test_set, **hp_d)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print('Test error rate: {}'.format(test_score))\n",
    "print('Total tack time(sec): {}'.format(total_time))\n",
    "print('Tact time per image(sec): {}'.format(total_time / len(test_list)))\n",
    "print('Confusion matrix: \\n{}'.format(confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO THIS FOR SEVERAL EXAMPLES\n",
    "\n",
    "# vec v.s. freeze v.s. se\n",
    "\n",
    "# restore trainval_list, test_list\n",
    "#file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "\n",
    "#trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict) # non-noisy dataset\n",
    "\n",
    "#test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "\n",
    "# FIXME\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict)\n",
    "save_dir = './compare/skc/' \n",
    "restore_dir = '/Data/checkpts/skc/model_fold_1_trainval_ratio_1.0.dnn'\n",
    "\n",
    "for idx_test in range(0, 10):\n",
    "    # Set a single test image\n",
    "\n",
    "    # # Re-sample a test instance\n",
    "    # test_list, _ = dataset.read_data_subset(root_dir, mode='validation1', sample_size=100)\n",
    "    # test_set = dataset.LazyDataset(root_dir, test_list, anno_dict)\n",
    "    \n",
    "    # Restore weights\n",
    "    net.logits.restore(restore_dir)\n",
    "\n",
    "    params = net.logits.parameters\n",
    "\n",
    "    name_test = test_list[idx_test]\n",
    "    img_test, lb_test = test_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.transpose(img_test,(1,2,0)))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['conv5_2'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "    # Calculate influence functions\n",
    "\n",
    "    # CG-FREEZE (1885 sec)\n",
    "    t1 = time.time()\n",
    "    ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':50})\n",
    "    IF_cg_logreg = IF_val(net, ihvp_cg_logreg, train_set)\n",
    "    print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "    np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "    #IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "    visualize_topk_samples(IF_cg_logreg, train_set, num_sample=5, save_path=save_path+'/cg-frz')\n",
    "    \n",
    "    # VECTOR-FREEZE (175 sec)\n",
    "    t1 = time.time()\n",
    "    IF_v_logreg = IF_val(net, v_logreg, train_set)\n",
    "    print('V_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_logreg),min(IF_v_logreg)]))\n",
    "    np.save(save_path+'/if_v_logreg.npy', IF_v_logreg)\n",
    "    #IF_v_logreg = np.load(save_path+'/if_v_logreg.npy')\n",
    "    visualize_topk_samples(IF_v_logreg, train_set, num_sample=5, save_path=save_path+'/vec-frz')\n",
    "\n",
    "    # Vector-FULL (1688 sec)\n",
    "    t1 = time.time()\n",
    "    IF_v = IF_val(net, v_test, train_set)\n",
    "    print('V takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v),min(IF_v)]))\n",
    "    np.save(save_path+'/if_v.npy', IF_v)\n",
    "    #IF_v = np.load(save_path+'/if_v.npy')\n",
    "    visualize_topk_samples(IF_v, train_set, num_sample=5, save_path=save_path+'/v')\n",
    "    \n",
    "    # VECTOR-FREEZE-cosine-similarity (178 sec)\n",
    "    t1 = time.time()\n",
    "    IF_v_cos = IF_val(net, v_logreg, train_set, cosine=True)\n",
    "    print('V_cos takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_cos),min(IF_v_cos)]))\n",
    "    np.save(save_path+'/if_v_cos.npy', IF_v_cos)\n",
    "    #IF_v_cos = np.load(save_path+'/if_v_cos.npy')\n",
    "    visualize_topk_samples(IF_v_cos, train_set, num_sample=5, save_path=save_path+'/vec-cos')\n",
    "\n",
    "#     # CG-FULL (1epoch, more than 3 hours, did it stuck at line search as it happened in ncg?)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set,**{'damping':0.1, 'maxiter':100})\n",
    "#     IF_cg = IF_val(net, ihvp_cg, train_set)\n",
    "#     print('CG takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg),min(IF_cg)]))\n",
    "#     np.save(save_path+'/if_cg.npy', IF_cg)\n",
    "#     visualize_topk_samples(IF_cg, train_set, num_sample=5, save_path=save_path+'/cg')\n",
    "    \n",
    "#     # SE-FULL (? sec: diverge)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':1e5, 'damping':0.1, 'batch_size':50, 'recursion_depth':100})\n",
    "#     IF_se = IF_val(net, ihvp_se, train_set)\n",
    "#     print('SE takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se),min(IF_se)]))\n",
    "#     np.save(save_path+'/if_se.npy', IF_se)\n",
    "#     visualize_topk_samples(IF_se, train_set, num_sample=5, save_path=save_path+'/se')\n",
    "    \n",
    "    # SE-FREEZE (1065 sec -> 11050)\n",
    "    t1 = time.time()\n",
    "    ihvp_se_logreg = get_inverse_hvp_se(net, net.loss, v_logreg, train_set,**{'scale':1e3, 'damping':0.1, 'batch_size':50, 'tolerance':0, 'recursion_depth':65})\n",
    "    IF_se_logreg = IF_val(net, ihvp_se_logreg, train_set)\n",
    "    print('SE_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se_logreg),min(IF_se_logreg)]))\n",
    "    np.save(save_path+'/if_se_logreg.npy', IF_se_logreg)\n",
    "    #IF_se_logreg = np.load(save_path+'/if_se_logreg.npy')\n",
    "    visualize_topk_samples(IF_se_logreg, train_set, num_sample=5, save_path=save_path+'/se-frz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate IF measure for each samples of each classes\n",
    "\n",
    "# WITH SEVERAL METHODOLOGIES\n",
    "\n",
    "temp_list, _ = dataset.read_data_subset(root_dir, mode='validation1')\n",
    "\n",
    "print('num_of_samples',len(temp_list))\n",
    "\n",
    "# restore trainval_list, test_list\n",
    "#file_dir = './compare/result_net_nn_if_nn/train_e_99502'\n",
    "#file_dir = './sample/result_net_nn_if_nn/train_B_69574'\n",
    "\n",
    "#trainval_list = list(np.load(file_dir+'/trainval_list.npy'))\n",
    "#test_list = list(np.load(file_dir+'/test_list.npy'))\n",
    "\n",
    "dic = {ks: [] for ks in anno_dict['classes'].values()}\n",
    "\n",
    "for tl in temp_list:\n",
    "    dic[tl.split('_')[0]].append(tl)\n",
    "\n",
    "sample_list = [dic[ks][0] for ks in dic.keys()]\n",
    "#sample_list = list(np.load('./sample/result_net_nn_if_nn/train_B_69574/sample_list.npy'))\n",
    "sample_set = dataset.LazyDataset(root_dir, sample_list, anno_dict)\n",
    "\n",
    "    \n",
    "print(len(sample_list), sample_list)\n",
    "\n",
    "save_dir = './sample/skc/result'\n",
    "restore_dir = '/Data/checkpts/skc/model_fold_1_trainval_ratio_1.0.dnn'\n",
    "train_set = dataset.LazyDataset(root_dir, trainval_list, anno_dict)\n",
    "\n",
    "\n",
    "for idx_test in range(len(sample_list)):\n",
    "    # Set a single test image\n",
    "\n",
    "    # Restore weights\n",
    "    net.logits.restore(restore_dir)\n",
    "\n",
    "    params = net.logits.parameters\n",
    "\n",
    "    name_test = sample_list[idx_test]\n",
    "    img_test, lb_test = sample_set.__getitem__(idx_test)\n",
    "    show_image_from_data(img_test)\n",
    "    v_test = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=params)\n",
    "    \n",
    "    lb_true = anno_dict['classes'][str(np.argmax(lb_test))]\n",
    "    lb_pred = anno_dict['classes'][str(np.argmax(net.logits.eval({net.X:img_test})))]\n",
    "    print('testfile name: ', name_test)\n",
    "    print('ground truth label: ', lb_true)\n",
    "    print('network prediction: ', lb_pred)\n",
    "\n",
    "    save_path = os.path.join(save_dir, name_test.split('.')[0])\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        # make folder\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    scipy.misc.imsave(save_path+'/test_reference_true_{}_pred_{}.png'.format(lb_true,lb_pred), np.transpose(img_test,(1,2,0)))\n",
    "\n",
    "    np.save(save_path+'/trainval_list', trainval_list)\n",
    "    np.save(save_path+'/test_list', test_list)\n",
    "    np.save(save_path+'/temp_list', temp_list)\n",
    "    np.save(save_path+'/sample_list', sample_list)\n",
    "\n",
    "    # CALCULATE IF WITH FREEZED NETWORK\n",
    "\n",
    "    params = net.loss.parameters\n",
    "    p_ftex = net.d['conv5_2'].parameters\n",
    "    p_logreg = tuple(set(params) - set(p_ftex)) # extract the weights of the last-layer (w,b)\n",
    "    print(p_logreg)\n",
    "    v_logreg = net.loss.grad({net.X:img_test, net.y:lb_test}, wrt=p_logreg)\n",
    "\n",
    "    # Calculate influence functions\n",
    "    ns = 10\n",
    "\n",
    "    # CG-FREEZE (1885 sec)\n",
    "    t1 = time.time()\n",
    "    ihvp_cg_logreg = get_inverse_hvp_cg(net, net.loss, v_logreg, train_set,**{'damping':0.0, 'maxiter':50})\n",
    "    IF_cg_logreg = IF_val(net, ihvp_cg_logreg, train_set)\n",
    "    print('CG_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg_logreg),min(IF_cg_logreg)]))\n",
    "    np.save(save_path+'/if_cg_logreg.npy', IF_cg_logreg)\n",
    "    #IF_cg_logreg = np.load(save_path+'/if_cg_logreg.npy')\n",
    "    visualize_topk_samples(IF_cg_logreg, train_set, num_sample=ns, save_path=save_path+'/cg-frz')\n",
    "    \n",
    "    # VECTOR-FREEZE (175 sec)\n",
    "    t1 = time.time()\n",
    "    IF_v_logreg = IF_val(net, v_logreg, train_set)\n",
    "    print('V_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_logreg),min(IF_v_logreg)]))\n",
    "    np.save(save_path+'/if_v_logreg.npy', IF_v_logreg)\n",
    "    #IF_v_logreg = np.load(save_path+'/if_v_logreg.npy')\n",
    "    visualize_topk_samples(IF_v_logreg, train_set, num_sample=ns, save_path=save_path+'/vec-frz')\n",
    "\n",
    "    # Vector-FULL (1688 sec)\n",
    "    t1 = time.time()\n",
    "    IF_v = IF_val(net, v_test, train_set)\n",
    "    print('V takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v),min(IF_v)]))\n",
    "    np.save(save_path+'/if_v.npy', IF_v)\n",
    "    #IF_v = np.load(save_path+'/if_v.npy')\n",
    "    visualize_topk_samples(IF_v, train_set, num_sample=ns, save_path=save_path+'/v')\n",
    "    \n",
    "    # VECTOR-FREEZE-cosine-similarity (178 sec)\n",
    "    t1 = time.time()\n",
    "    IF_v_cos = IF_val(net, v_logreg, train_set, cosine=True)\n",
    "    print('V_cos takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_v_cos),min(IF_v_cos)]))\n",
    "    np.save(save_path+'/if_v_cos.npy', IF_v_cos)\n",
    "    #IF_v_cos = np.load(save_path+'/if_v_cos.npy')\n",
    "    visualize_topk_samples(IF_v_cos, train_set, num_sample=ns, save_path=save_path+'/vec-cos')\n",
    "\n",
    "#     # CG-FULL (1epoch, more than 3 hours, did it stuck at line search as it happened in ncg?)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_cg = get_inverse_hvp_cg(net, net.loss, v_test, train_set,**{'damping':0.1, 'maxiter':100})\n",
    "#     IF_cg = IF_val(net, ihvp_cg, train_set)\n",
    "#     print('CG takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_cg),min(IF_cg)]))\n",
    "#     np.save(save_path+'/if_cg.npy', IF_cg)\n",
    "#     visualize_topk_samples(IF_cg, train_set, num_sample=ns, save_path=save_path+'/cg')\n",
    "    \n",
    "#     # SE-FULL (? sec: diverge)\n",
    "#     t1 = time.time()\n",
    "#     ihvp_se = get_inverse_hvp_se(net, net.loss, v_test, train_set,**{'scale':1e5, 'damping':0.1, 'batch_size':50, 'recursion_depth':100})\n",
    "#     IF_se = IF_val(net, ihvp_se, train_set)\n",
    "#     print('SE takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se),min(IF_se)]))\n",
    "#     np.save(save_path+'/if_se.npy', IF_se)\n",
    "#     visualize_topk_samples(IF_se, train_set, num_sample=ns, save_path=save_path+'/se')\n",
    "    \n",
    "    # SE-FREEZE (1065 sec)\n",
    "    t1 = time.time()\n",
    "    ihvp_se_logreg = get_inverse_hvp_se(net, net.loss, v_logreg, train_set,**{'scale':1e3, 'damping':0.1, 'batch_size':50, 'tolerance':0, 'recursion_depth':65})\n",
    "    IF_se_logreg = IF_val(net, ihvp_se_logreg, train_set)\n",
    "    print('SE_logreg takes {} sec, and its max/min value {}'.format(time.time()-t1, [max(IF_se_logreg),min(IF_se_logreg)]))\n",
    "    np.save(save_path+'/if_se_logreg.npy', IF_se_logreg)\n",
    "    #IF_se_logreg = np.load(save_path+'/if_se_logreg.npy')\n",
    "    visualize_topk_samples(IF_se_logreg, train_set, num_sample=ns, save_path=save_path+'/se-frz')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skc dataset\n",
    "\n",
    "학습시간: 약 7000sec\n",
    "\n",
    "전체 데이터셋 사용 시:(55647장 사용 시) se 65 epoch 11050 (약 3시간 걸림)\n",
    "너무 오래 걸려서 10000장으로 줄임.\n",
    "\n",
    "CG-frz: 2528, 3175 sec\n",
    "V-frz: 72 sec\n",
    "V-full: 305 sec\n",
    "V-cos: 72 sec\n",
    "SE-frz: 2067 sec (but not yet converged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
